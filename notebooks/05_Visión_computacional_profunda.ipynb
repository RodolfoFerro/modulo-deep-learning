{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/modulo-deep-learning/blob/main/notebooks/05_Visi%C3%B3n_computacional_profunda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jO_1gISKxk"
      },
      "source": [
        "# **Aprendizaje profundo - Sesi√≥n 5  üß†**\n",
        "\n",
        "> **Descripci√≥n:** Cuaderno de contenidos del m√≥dulo de aprendizaje profundo para el Dimplomado en Ciencia de Datos de la ENES UNAM Le√≥n, 2024. <br>\n",
        "> **Autor:** [Rodolfo Ferro](https://github.com/RodolfoFerro) <br>\n",
        "> **Contacto:** [ferro@cimat.mx](mailto:ferro@cimat.mx)\n",
        "\n",
        "## Contenido\n",
        "\n",
        "### Pre√°mbulo\n",
        "\n",
        "- Introducci√≥n a la reconstrucci√≥n de im√°genes\n",
        "\n",
        "### Secci√≥n I\n",
        "\n",
        "1. Introducci√≥n a los autoencoders\n",
        "2. Estructura b√°sica de un autoencoder\n",
        "3. Funcionamiento de un autoencoder\n",
        "\n",
        "\n",
        "### Secci√≥n II\n",
        "\n",
        "4. Limitaciones de los autoencoders b√°sicos\n",
        "5. Introducci√≥n a los autoencoders convolucionales\n",
        "6. Estructura de los autoencoders convolucionales\n",
        "\n",
        "### Secci√≥n III\n",
        "\n",
        "7. Implementaci√≥n y entrenamiento de los autoencoders convolucionales\n",
        "8. Aplicaciones de los autoencoders convolucionales en el denoising de im√°genes\n",
        "9. Trabajos relacionados y avances recientes\n",
        "\n",
        "### Secci√≥n IV - ¬°Extra!\n",
        "\n",
        "10. Segmentaci√≥n de im√°genes\n",
        "11. Implementaci√≥n de U-Net\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNVG2PnSEtQN"
      },
      "source": [
        "## **Pre√°mbulo**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducci√≥n a la reconstrucci√≥n de im√°genes\n",
        "\n",
        "\n",
        "- Proceso de generar una imagen de salida a partir de una de entrada, generalmente con el objetivo de **restaurar o mejorar la calidad de la imagen original**.\n",
        "- En el contexto de los autoencoders y la tarea de denoising, la reconstrucci√≥n implica generar una versi√≥n limpia y libre de ruido de una imagen ruidosa o de baja calidad.\n",
        "\n",
        "Es √∫til y ayuda para:\n",
        "- Restauraci√≥n de la calidad visual.\n",
        "- Preservaci√≥n de la informaci√≥n relevante.\n",
        "- Mejora de aplicaciones y an√°lisis (eliminaci√≥n de ruido).\n",
        "- Preservaci√≥n de caracter√≠sticas y texturas.\n",
        "- Calidad y experiencia visual.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*ZE8YXDof_MVxk43k.png\" width=\"50%\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "IOOWWpUv32iz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS_zSehl30t1"
      },
      "source": [
        "## **Secci√≥n I**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducci√≥n a los autoencoders\n",
        "\n",
        "Son ANNs utilizadas para aprender representaciones eficientes de los datos de entrada sin necesidad de etiquetas o supervisi√≥n externa."
      ],
      "metadata": {
        "id": "2k4ZBY7drHpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estructura b√°sica y funcionamiento de un autoencoder\n",
        "\n",
        "Est√°n dise√±adas para aprender representaciones eficientes de los datos de entrada mediante un proceso de compresi√≥n y descompresi√≥n.\n",
        "\n",
        "Las partes fundamentales son el encoder y el decoder.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png\" width=\"70%\">\n",
        "</center>\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "- **Encoder:** Toma los datos de entrada y los transforma en una representaci√≥n de menor dimensionalidad, tambi√©n conocida como representaci√≥n latente.\n",
        "- **El objetivo del codificador es‚Ä¶** Comprimir la informaci√≥n esencial de los datos en esta representaci√≥n latente.\n",
        "- **Usualmente**, el codificador est√° compuesto por capas de neuronas que reducen gradualmente la dimensionalidad de los datos a medida que se propagan a trav√©s de la red. Este proceso de reducci√≥n dimensional es crucial para extraer las caracter√≠sticas m√°s importantes de los datos y deshacerse de la informaci√≥n redundante o ruidosa.\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "- **Decoder:** Toma la representaci√≥n latente generada y la reconstruye en una salida que se asemeja lo m√°s posible a la entrada original.\n",
        "- **El objetivo del decodificador es‚Ä¶** Descomprimir la representaci√≥n latente y generar una reconstrucci√≥n fiel de los datos de entrada.\n",
        "- **Al igual que el encoder**, el decodificador est√° compuesto por capas de neuronas, pero en este caso, las capas aumentan gradualmente la dimensionalidad de la representaci√≥n latente hasta que se obtiene una salida de la misma dimensi√≥n que los datos de entrada originales.\n",
        "\n",
        "**La idea central es que**, al restringir la capacidad de reconstrucci√≥n de la red, se obliga al codificador a aprender representaciones m√°s compactas y significativas de los datos.\n",
        "\n",
        "**En otras palabras**, se busca que el codificador capture las caracter√≠sticas m√°s importantes y relevantes de los datos en la representaci√≥n latente, mientras que el decodificador se encarga de reconstruir los datos de entrada a partir de esa representaci√≥n latente.\n",
        "\n",
        "#### Aplicaicones de autoencoders\n",
        "\n",
        "- **Denoising de im√°genes:** Se utilizan para eliminar el ruido de las im√°genes y reconstruir versiones m√°s limpias.\n",
        "Generaci√≥n de contenido: Pueden generar contenido nuevo y original aprendiendo las caracter√≠sticas latentes de un conjunto de datos. Esto se utiliza en aplicaciones como la generaci√≥n de im√°genes sint√©ticas, la creaci√≥n de m√∫sica o la generaci√≥n de texto.\n",
        "- **Detecci√≥n de anomal√≠as:** Se utilizan para detectar patrones anormales o inusuales en los datos. Esto se aplica en √°reas como la detecci√≥n de fraudes en transacciones financieras, la detecci√≥n de intrusiones en sistemas de seguridad o la detecci√≥n de anomal√≠as en im√°genes m√©dicas.\n",
        "- **Reducci√≥n de dimensionalidad:** Los autoencoders se utilizan para reducir la dimensionalidad de los datos, lo que facilita la visualizaci√≥n y comprensi√≥n de datos complejos. Esto es √∫til en an√°lisis exploratorio de datos, visualizaci√≥n de datos de gran dimensi√≥n y clustering.\n",
        "\n",
        "#### Aprendizaje\n",
        "\n",
        "- **Aprendizaje:** Durante el entrenamiento de los autoencoders, se utilizan pares de datos de entrada y salida correspondientes.\n",
        "- La red se entrena para **minimizar** la diferencia entre la entrada original y la salida reconstruida.."
      ],
      "metadata": {
        "id": "5dy-RQOw4kQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "1kb3YuuW51-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los datos."
      ],
      "metadata": {
        "id": "1RbMybth8xL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ],
      "metadata": {
        "id": "58k-OiKs7WQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[0], cmap='gray')"
      ],
      "metadata": {
        "id": "Hr5CjhCK-Asn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la clase Autoencoder con TensorFlow."
      ],
      "metadata": {
        "id": "_9VwrJXJ8YSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Construimos el encoder\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: A√±ade una capa Flatten\n",
        "\n",
        "            # TODO: A√±ade una capa Dense -> latent_dim, ReLU\n",
        "\n",
        "        ])\n",
        "\n",
        "        # Construimos el decoder\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: A√±ade una capa Dense -> 784, Sigmoid\n",
        "\n",
        "            # TODO: A√±ade una capa Reshape -> (28, 28)\n",
        "\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        # Esta funci√≥n nos permite invocar los sub modelos\n",
        "        # para poder hacer inferencia sobre cada uno (predict)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "1qeFNIHw7i4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos par√°metros y creamos un autoencoder."
      ],
      "metadata": {
        "id": "Kx5QY8uY8qSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos dimensi√≥n de espacio latente\n",
        "latent_dim = 64\n",
        "\n",
        "# Instanciamos un autoencoder\n",
        "autoencoder = Autoencoder(latent_dim)"
      ],
      "metadata": {
        "id": "ZO8FSy2S7pXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos y entrenamos."
      ],
      "metadata": {
        "id": "GjfNJhov8tYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "                          epochs=10,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "Si_2tc2u8is5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "mZexUsya-dwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "seen = 'loss' # or 'loss'\n",
        "\n",
        "hist_values = history.history[seen]\n",
        "hist_values_val = history.history['val_' + seen]\n",
        "eje_x = np.arange(len(hist_values))\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=eje_x,\n",
        "                         y=hist_values,\n",
        "                         mode='lines',\n",
        "                         name=seen))\n",
        "fig.add_trace(go.Scatter(x=eje_x,\n",
        "                         y=hist_values_val,\n",
        "                         mode='lines',\n",
        "                         name='val_' + seen))\n",
        "fig.update_layout(title='Historia de entrenamiento',\n",
        "                   xaxis_title='√âpocas',\n",
        "                   yaxis_title=seen.capitalize())\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-YNmdDVIFwRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploramos los resultados."
      ],
      "metadata": {
        "id": "sUdWJBEO8vC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ],
      "metadata": {
        "id": "Y8ZfJ8J__j5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "    # Mostramos imagen original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i + 121])\n",
        "    plt.title(\"Original\")\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos imagen reconstruida\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i + 121])\n",
        "    plt.title(\"Reconstruida\")\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DF9NFmQ-7q_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reto:** ¬øPuedes mejorar a√∫n m√°s las reconstrucciones?\n",
        "\n",
        "Te recomiendo explorar lo siguiente:\n",
        "- Modifica el n√∫mero de capas y neuronas por capa. Recuerda que debe ir disminuyendo en el encoder y aumentando en el decoder y deben tener una estructura espejeada.\n",
        "- Modifica el n√∫mero de √©pocas de entrenamiento.\n",
        "- ¬øQuieres intentar reconstruir cosas con convoluciones? Te invito a que lo intentes.\n",
        "\n",
        "\n",
        "**Lecturas recomendadas:**\n",
        "- [A 2021 Guide to improving CNNs-Optimizers: Adam vs SGD](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)"
      ],
      "metadata": {
        "id": "7r1Tx5AyBAK3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtpuj0PBqWYc"
      },
      "source": [
        "## **Secci√≥n II**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Limitaciones de autoencoders b√°sicos\n",
        "\n",
        "- **Incapacidad para capturar informaci√≥n espacial:** Los autoencoders b√°sicos pueden tener dificultades para capturar la estructura espacial de las im√°genes, ya que no tienen en cuenta la informaci√≥n de vecindad de los p√≠xeles.\n",
        "- **Sensibilidad a las transformaciones:** Pueden ser sensibles a las transformaciones geom√©tricas, como la rotaci√≥n o el desplazamiento de la imagen, lo que puede afectar su capacidad de reconstrucci√≥n.\n",
        "- **Autoencoders convolucionales como soluci√≥n:** Son una variante de los autoencoders que incorporan capas convolucionales para abordar las limitaciones mencionadas.\n",
        "- **Ventajas de los autoencoders convolucionales:** Mejoran con la capacidad para capturar algunas caracter√≠sticas espaciales, preservar la estructura y ser m√°s robustos frente a transformaciones geom√©tricas\n",
        "\n"
      ],
      "metadata": {
        "id": "jJdHo17RAp4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoders convolucionales\n",
        "\n",
        "La estructura de un autoencoder convolucional consta de dos partes principales: el codificador (encoder) y el decodificador (decoder). Cada una de estas partes est√° compuesta por capas convolucionales, capas de muestreo y, en algunos casos, capas de convoluci√≥n transpuesta o de upsampling.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*TOJD69Y8dZsKFEW-21xUPg.png\" width=\"70%\">\n",
        "</center>\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "- **Capas convolucionales:** Estas capas utilizan filtros convolucionales para extraer caracter√≠sticas de nivel superior de la entrada.\n",
        "- **Capas de pooling:** Estas capas reducen la dimensionalidad de las caracter√≠sticas extra√≠das al realizar un muestreo o reducci√≥n de tama√±o, como el max pooling.\n",
        "- **Funciones de activaci√≥n:** Despu√©s de cada capa convolucional y de muestreo, se aplica una funci√≥n de activaci√≥n no lineal, como la funci√≥n ReLU (Rectified Linear Unit), para introducir no linealidad en la red.\n",
        "\n",
        "#### Espacio latente\n",
        "\n",
        "- **Capa de aplanamiento:** Antes de llegar al espacio latente, la salida del codificador se aplanar√° en un vector unidimensional.\n",
        "- **Capa densa (fully connected):** La capa densa o fully connected reduce a√∫n m√°s la dimensionalidad y mapea las caracter√≠sticas a un espacio latente de menor dimensi√≥n. Esta capa suele tener una funci√≥n de activaci√≥n, como la ReLU o la tangente hiperb√≥lica.\n",
        "\n",
        "#### Decoder\n",
        "\n",
        "- **Capas densas (fully connected):** En el decodificador, se utilizan capas densas para aumentar gradualmente la dimensionalidad del espacio latente y reconstruir las caracter√≠sticas originales.\n",
        "- **Capas de convoluci√≥n transpuesta o upsampling:** Estas capas realizan la operaci√≥n inversa de las capas de muestreo, aumentando gradualmente el tama√±o espacial de las caracter√≠sticas.\n",
        "- **Capas de convoluci√≥n:** Al final del decodificador, se utilizan capas convolucionales para generar una salida final con las mismas dimensiones que la entrada original.\n",
        "- **Funci√≥n de activaci√≥n final:** La funci√≥n de activaci√≥n final depende del rango de valores de la imagen de salida. Por ejemplo, en im√°genes en escala de grises, se puede utilizar una funci√≥n de activaci√≥n sigmoide para obtener valores entre 0 y 1.\n",
        "\n",
        "La estructura del autoencoder convolucional puede variar seg√∫n la tarea espec√≠fica y los requisitos del problema. Se pueden agregar capas adicionales, como capas de regularizaci√≥n, capas de normalizaci√≥n o capas de convoluci√≥n dilatadas, para mejorar el rendimiento y la capacidad de generalizaci√≥n del modelo."
      ],
      "metadata": {
        "id": "Vs8JZj5D6Jgx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KKqdPJBCU_E"
      },
      "source": [
        "## **Secci√≥n III**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Implementaci√≥n y entrenamiento de los autoencoders convolucionales"
      ],
      "metadata": {
        "id": "WCO2eFvsCOVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Denoise(Model):\n",
        "    def __init__(self):\n",
        "        super(Denoise, self).__init__()\n",
        "\n",
        "        # Creamos el encoder convolucional\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: A√±ade una capa Input -> shape=(28, 28, 1)\n",
        "\n",
        "            # TODO: A√±ade una capa Conv2D -> 16, (3, 3), activation='relu', padding='same', strides=2\n",
        "\n",
        "            # TODO: A√±ade una capa Conv2D -> 8, (3, 3), activation='relu', padding='same', strides=2\n",
        "\n",
        "        ])\n",
        "\n",
        "        # Creamos el decoder convolucional\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "\n",
        "            # TODO: A√±ade una capa Conv2DTranspose -> 8, kernel_size=3, activation='relu', padding='same', strides=2\n",
        "\n",
        "            # TODO: A√±ade una capa Conv2DTranspose -> 16, kernel_size=3, activation='relu', padding='same', strides=2\n",
        "\n",
        "            # TODO: A√±ade una capa Conv2D -> 1, (3, 3), activation='sigmoid', padding='same'\n",
        "\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        # Esta funci√≥n nos permite invocar los sub modelos\n",
        "        # para poder hacer inferencia sobre cada uno (predict)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "stCt0xNzzk_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lecturas recomendadas:**\n",
        "\n",
        "- [Convolution, Padding, Stride, and Pooling in CNN](https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26)\n",
        "- [Why do we need conv2d_transpose?\n",
        "](https://medium.com/@vaibhavshukla182/why-do-we-need-conv2d-transpose-2534cd2a4d98)\n",
        "- [Deconvolution](https://vincmazet.github.io/bip/restoration/deconvolution.html)\n",
        "- [Image Segmentation using deconvolution layer in Tensorflow](https://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/)"
      ],
      "metadata": {
        "id": "l2OVuOuwHqv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicaciones de los autoencoders convolucionales en el denoising de im√°genes\n",
        "\n",
        "Para poder explorar un ejemplo de aplicaci√≥n de autoencoders donde los utilicemos para quitar el ruido de alunas im√°genes, volveremos a utilizar el dataset de modas y le agregaremos algo de ruido a las im√°genes. De este modo, entrenaremos a la red para que aprenda c√≥mo debe verse una imagen a partir de una con ruido.\n",
        "\n",
        "Cargaremos y preprocesaremos nuevamente los datos."
      ],
      "metadata": {
        "id": "aMBsYpdK6cIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "erEOQdn0C5x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos ruido a los datos."
      ],
      "metadata": {
        "id": "yepxPRZ0DEPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)\n",
        "\n",
        "x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n",
        "x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)"
      ],
      "metadata": {
        "id": "NyGaNXfnDDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos explorar c√≥mo se ven los datos con algo de ruido."
      ],
      "metadata": {
        "id": "IaWoRWDsDkou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.title(\"Original + Ruido\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
        "    plt.gray()\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.grid(False)"
      ],
      "metadata": {
        "id": "31WxIj_jDkQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciaremos, compilaremos y entrenaremos un nuevo autoencoder llamado \"denoiser\"."
      ],
      "metadata": {
        "id": "vziHzY1CDc7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser = Denoise()"
      ],
      "metadata": {
        "id": "quxg9yeHAdR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = denoiser.fit(x_train_noisy, x_train,\n",
        "                       epochs=10,\n",
        "                       shuffle=True,\n",
        "                       validation_data=(x_test_noisy, x_test))"
      ],
      "metadata": {
        "id": "38IRvJY17Ivb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "seen = 'loss' # or 'loss'\n",
        "\n",
        "hist_values = history.history[seen]\n",
        "hist_values_val = history.history['val_' + seen]\n",
        "eje_x = np.arange(len(hist_values))\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=eje_x,\n",
        "                         y=hist_values,\n",
        "                         mode='lines',\n",
        "                         name=seen))\n",
        "fig.add_trace(go.Scatter(x=eje_x,\n",
        "                         y=hist_values_val,\n",
        "                         mode='lines',\n",
        "                         name='val_' + seen))\n",
        "fig.update_layout(title='Historia de entrenamiento',\n",
        "                   xaxis_title='√âpocas',\n",
        "                   yaxis_title=seen.capitalize())\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oMRtH2wTFMPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos explorar los detalles de ambos submodelos."
      ],
      "metadata": {
        "id": "USi-p3n5Dsvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser.encoder.summary()"
      ],
      "metadata": {
        "id": "E8UXlSb0DiBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denoiser.decoder.summary()"
      ],
      "metadata": {
        "id": "5LKiHWpSDv_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos la inferencia sobre las im√°genes ruidosas utilizando ambos modelos."
      ],
      "metadata": {
        "id": "KRJUL90BD0Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs = denoiser.encoder(x_test_noisy).numpy()\n",
        "decoded_imgs = denoiser.decoder(encoded_imgs).numpy()"
      ],
      "metadata": {
        "id": "GzQ4GoZFDxr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # Mostramos original + ruido\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.title(\"Original + Ruido\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i + 121]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos reconstrucci√≥n\n",
        "    bx = plt.subplot(3, n, i + n + 1)\n",
        "    plt.title(\"Reconstruida\")\n",
        "    plt.imshow(tf.squeeze(decoded_imgs[i + 121]))\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Mostramos original\n",
        "    cx = plt.subplot(3, n, i + n + n + 1)\n",
        "    plt.title(\"Original\")\n",
        "    plt.imshow(tf.squeeze(x_test[i + 121]))\n",
        "    plt.gray()\n",
        "    cx.get_xaxis().set_visible(False)\n",
        "    cx.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.savefig('imagen_sin_ruido.png', dpi=300)"
      ],
      "metadata": {
        "id": "SdIqgkt1Dzvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reto:** ¬øPuedes mejorar a√∫n m√°s el modelo?\n",
        "\n",
        "Te recomiendo explorar lo siguiente:\n",
        "- Modifica el n√∫mero de capas y par√°metros de convoluci√≥n por capa.\n",
        "- Modifica el n√∫mero de √©pocas de entrenamiento.\n",
        "- Puedes explorar el remover ruido de im√°genes con m√°s canales (im√°genes RGB) utilizando otros datasets."
      ],
      "metadata": {
        "id": "8U1tdrMYlDen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trabajos relacionados y avances recientes\n",
        "\n",
        "\n",
        "Ha habido varios trabajos de investigaci√≥n y avances recientes que han contribuido al desarrollo de nuevas arquitecturas, t√©cnicas de entrenamiento mejoradas y aplicaciones emergentes.\n",
        "\n",
        "- **UNet:** Es ampliamente utilizada en el campo de la segmentaci√≥n de im√°genes, pero tambi√©n se ha aplicado con √©xito en tareas de denoising.\n",
        "- **Variational Autoencoders (VAEs):** Los VAEs son una variante de los autoencoders que se utilizan para el aprendizaje de distribuciones latentes. Han demostrado ser efectivos en el denoising de im√°genes al aprender representaciones latentes que siguen una distribuci√≥n probabil√≠stica, lo que permite una generaci√≥n m√°s controlada y realista de im√°genes limpias.\n",
        "- **GANs y Autoencoders Generativos (GANs-AE):** La combinaci√≥n de las GANs y los AE ha llevado al desarrollo de los GANs-AE. Estos modelos aprovechan la capacidad de los GANs para generar im√°genes realistas y los autoencoders para aprender representaciones latentes eficientes. Los GANs-AE han demostrado ser efectivos en el denoising y la generaci√≥n de im√°genes de alta calidad.\n",
        "\n",
        "\n",
        "#### **Tareas en el campo de la visi√≥n artificial**\n",
        "\n",
        "1. **Clasificaci√≥n de im√°genes:** La tarea de clasificaci√≥n de im√°genes implica asignar una etiqueta o categor√≠a a una imagen de entrada. Esto implica entrenar un modelo para reconocer y distinguir diferentes objetos, personas o escenas en una imagen.\n",
        "2. **Detecci√≥n de objetos:** La detecci√≥n de objetos implica localizar y clasificar m√∫ltiples objetos en una imagen. El objetivo es detectar la presencia y la ubicaci√≥n de objetos espec√≠ficos en una escena, a menudo utilizando cuadros delimitadores para delinear las regiones donde se encuentran los objetos.\n",
        "3. **_Denoising_ o reconstrucci√≥n de im√°genes:** Consiste en eliminar o reducir el ruido presente en una imagen, obteniendo una versi√≥n m√°s limpia y clara. Esta tarea es relevante en √°reas como la fotograf√≠a, la medicina y la seguridad..\n",
        "4. **Segmentaci√≥n sem√°ntica:** La segmentaci√≥n sem√°ntica implica asignar una etiqueta a cada p√≠xel de una imagen para identificar y delimitar las diferentes regiones o objetos presentes. El objetivo es comprender la estructura y el contenido de una imagen a nivel de p√≠xel.\n",
        "5. **Detecci√≥n de rostros:** La detecci√≥n de rostros es una tarea espec√≠fica de la visi√≥n artificial que implica detectar y localizar los rostros en una imagen. Es ampliamente utilizado en aplicaciones de reconocimiento facial, an√°lisis de emociones y sistemas de seguridad.\n",
        "6. **Reconocimiento y verificaci√≥n facial:** El reconocimiento facial se refiere a la tarea de identificar y reconocer a una persona espec√≠fica a partir de una imagen o secuencia de im√°genes. La verificaci√≥n facial se enfoca en verificar si una imagen de rostro coincide con una identidad espec√≠fica.\n",
        "7. **Estimaci√≥n de pose:** La estimaci√≥n de pose se refiere a la tarea de determinar la posici√≥n y orientaci√≥n de un objeto o persona en una imagen. Esto implica detectar y rastrear las articulaciones o puntos clave en una imagen para comprender la postura y el movimiento.\n",
        "8. **Estimaci√≥n de profundidad:** La estimaci√≥n de profundidad implica inferir la informaci√≥n de la distancia o la profundidad de los objetos en una imagen. Es √∫til en aplicaciones de realidad virtual, conducci√≥n aut√≥noma y sistemas de navegaci√≥n.\n",
        "9. **Super-resoluci√≥n:** La super-resoluci√≥n se refiere a aumentar la resoluci√≥n o la calidad de una imagen de baja resoluci√≥n. El objetivo es generar una versi√≥n de alta resoluci√≥n que capture m√°s detalles y claridad.\n"
      ],
      "metadata": {
        "id": "JP3NGkSg6fO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Secci√≥n IV - ¬°Extra!**"
      ],
      "metadata": {
        "id": "YTPgeeh5JHL2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_BXRzLrBEdd"
      },
      "source": [
        "### **Clonamos el repositorio**\n",
        "\n",
        "Comenzaremos clonando el repositorio y asignando a la carpeta como la ra√≠z."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p7Tqyz-zfti"
      },
      "source": [
        "!git clone https://github.com/RodolfoFerro/modulo-deep-learning.git\n",
        "%cd modulo-deep-learning\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8IkF4N2DJV6"
      },
      "source": [
        "La estructura del c√≥digo fuente es como sigue:\n",
        "- `tools/unet/model.py` - Contiene la implementaci√≥n del U-Net.\n",
        "- `tools/data.py` - Contiene funciones de utiler√≠a para carga de datos.\n",
        "- `tools/image.py` - Coniene funciones de utiler√≠a para cargar im√°genes y mostrar los resultados de las inferencias.\n",
        "- `main.py` - Contiene una sencilla implementaci√≥n de este cuaderno en un script de Python para entrenar el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA1wiuc01lQd"
      },
      "source": [
        "### **Cargamos los datos**\n",
        "\n",
        "A continuaci√≥n procedemos a importar algunas bibliotecas y el c√≥digo base del modelo.\n",
        "\n",
        "Haremos uso de alunas funciones que permiten cargar datos que encuentras en el folder `data`.\n",
        "\n",
        "Comenzaremos importando las funciones de los m√≥dulos a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kdal-XjznDC"
      },
      "source": [
        "from tools.data import train_generator\n",
        "from tools.data import test_generator\n",
        "from tools.data import save_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLq_Gu9hDxX4"
      },
      "source": [
        "Procedemos a crear un diccionario de configuraci√≥n para cargar datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGXyhvn7bW0h"
      },
      "source": [
        "data_gen_args = dict(\n",
        "    rotation_range=0.2,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.05,\n",
        "    zoom_range=0.05,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_gen = train_generator(\n",
        "    2, 'data/membrane/train',\n",
        "    'image', 'label',\n",
        "    data_gen_args,\n",
        "    save_to_dir=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Creamos el modelo**\n",
        "\n",
        "Ahora, procederemos a crear el modelo. Para ello, dos opciones ser√°n previstas.\n",
        "\n",
        "**OPCI√ìN A:** Creamos nuestro propio U-Net con nuestras propias caracter√≠sticas, bas√°ndonos en la propuesta original:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png\" width=\"60%\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "K87oTtdQhdui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def unet(pretrained_weights=None, input_size=(256, 256, 1)):\n",
        "    \"\"\"U-Net model constructor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pretrained_weights : str\n",
        "        Path to pretrained weights.\n",
        "    input_size : tuple\n",
        "        Spatial size of the expected input image.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Convolution chain #1\n",
        "    # conv_1 = ...\n",
        "\n",
        "    # Continua aqu√≠ con tu propia implementaci√≥n..."
      ],
      "metadata": {
        "id": "QAH4Q0TxiSnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttz1pS1SD5su"
      },
      "source": [
        "**OPCI√ìN B:** Creamos una instancia del modelo ya implementado y entrenamos con los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKZb95kubdUf"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "from tools.unet.model import unet\n",
        "\n",
        "# Replicable experiments\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = unet()\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'unet_membrane.keras',\n",
        "    monitor='loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=300,\n",
        "    epochs=5,\n",
        "    callbacks=[model_checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "Xz0G99UGYndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "losses = history.history['loss']\n",
        "eje_x = np.arange(len(losses))\n",
        "\n",
        "fig = px.line(\n",
        "    x=eje_x,\n",
        "    y=losses,\n",
        "    title='Historia de entrenamiento',\n",
        "    labels=dict(x='√âpocas', y='Error')\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "jodtKIlZMOSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edeZ8WvsD89n"
      },
      "source": [
        "**Hasta este punto deber√≠as haber entrenado exitosamente un U-Net con algunas im√°genes m√©dicas.**\n",
        "\n",
        "Una vez entrenado el modelo, podemos realizar pruebas de inferencia con el conjunto de pruebas que se encuentra en la misma carpeta de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i_ECOlzefxU"
      },
      "source": [
        "test_gen = test_generator('data/membrane/test')\n",
        "results = model.predict(test_gen, verbose=True)\n",
        "save_results('data/results', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2-wE6B7jFj4"
      },
      "source": [
        "\n",
        "### **Resultados gr√°ficos**\n",
        "\n",
        "El c√≥digo base provee algunas funciones para cargar, inferir y crear m√°scaras de los resultados al trabajar sobre algunas im√°genes.\n",
        "\n",
        "Procedemos a importar las funciones del m√≥dulo de im√°genes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU8bP3t25TDm"
      },
      "source": [
        "from tools.image import load_test_image\n",
        "from tools.image import inference_over_image\n",
        "from tools.image import create_mask\n",
        "from tools.image import overlay_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRE71y6dEchq"
      },
      "source": [
        "Cargamos una imagen del directorio de prueba, especificando con un n√∫mero entero el √≠ndice de alguna de las 30 im√°genes (`[0, 29]`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzjLv6Rd8KuB"
      },
      "source": [
        "img = load_test_image(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUkAB_jOEpGz"
      },
      "source": [
        "Usamos el modelo previamente entrenado para inferir sobre la imagen previamente cargada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugcpTWtHp_rZ"
      },
      "source": [
        "out = inference_over_image(model, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4rdm-4Ey2H"
      },
      "source": [
        "Creamos una m√°scara a partir de la inferencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNylyGmZ0xEw"
      },
      "source": [
        "mask = create_mask(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjSK__TE1j8"
      },
      "source": [
        "Sobreponemos la m√°scara en la im√°gen original para validar el resulatdo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Q4oxihCgSq"
      },
      "source": [
        "res = overlay_mask(img, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsmVIiyTE51p"
      },
      "source": [
        "**¬°Felicidades! Has utilizado exitosamente tu modelo entrenado sobre algunas im√°genes m√©dicas.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reto:** Yo me he encargado de enfocarme en utilizar las detecciones para la identificaci√≥n de pared celular, sin embargo, puedes modificar o crear tus propias funciones para la detecci√≥n celular completa.\n",
        "\n",
        "Por otro lado, no debes limitarte a ello, sino que puedes crear o cargar tu propio conjunto de datos para segmentar otro tipo de elementos, como las mitocondrias (echa un vistazo al [Electron Microscopy Dataset](https://www.epfl.ch/labs/cvlab/data/data-em/))."
      ],
      "metadata": {
        "id": "c-g406-ALN_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "\n",
        "> Contenido creado por **Rodolfo Ferro**, 2024. <br>\n",
        "> Para cualquier retroalimentaci√≥n, puedes contactarme a trav√©s del correo [ferro@cimat.mx](mailto:ferro@cimat.mx)."
      ],
      "metadata": {
        "id": "hSdbQU3e6-Ky"
      }
    }
  ]
}