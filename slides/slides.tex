\documentclass[10pt,border=3pt,tikz]{beamer}

\usepackage{pgfpages}
\usepackage[T1]{fontenc}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{etoolbox}
\usepackage{listofitems} % for \readlist to create arrays
\tikzstyle{mynode}=[thick,draw=blue,fill=blue!20,circle,minimum size=15]
%\setbeameroption{show notes on second screen }
\usetheme[
% nojauge,
% nomail,
% rule,
delaunay,
amurmapleblack
]{Amurmaple}

\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{emoji}
\graphicspath{ {./images/} }

\usepackage{xcolor}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}
\tikzset{
    >=latex, % for default LaTeX arrow head
    node/.style={thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6},
    node in/.style={node,green!20!black,draw=mygreen!30!black,fill=mygreen!25},
    node hidden/.style={node,blue!20!black,draw=myblue!30!black,fill=myblue!20},
    node convol/.style={node,orange!20!black,draw=myorange!30!black,fill=myorange!20},
    node out/.style={node,red!20!black,draw=myred!30!black,fill=myred!20},
    connect/.style={thick,mydarkblue}, %,line cap=round
    connect arrow/.style={-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1},
    node 1/.style={node in}, % node styles, numbered for easy mapping with \nstyle
    node 2/.style={node hidden},
    node 3/.style={node out}
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3

\definecolor{newremark}{rgb}{0.7,0.2,0.2}
\colorlet{AmurmapleRemarkColor}{newremark}

\title[Deep Learning]{Aprendizaje profundo}
\author[R.~Ferro (@rodo\_ferro)]{Rodolfo Ferro}
\subtitle{Módulo 5}
\institute[ENES Unidad León]{Diplomado en Ciencia de Datos\\
	Escuela Nacional de Estudios Superiores, Unidad León}
\date{Agosto, 2024}
\titlegraphic{\includegraphics[width=3cm]{logo.png}}
\mail{ferro@cimat.mx}
\webpage{https://rodolfoferro.xyz}
% \collaboration{in collaboration with \LaTeX{}}
\logo{\includegraphics[width=1.6cm]{logo.png}}

\begin{document}
    
	\maketitle
    
        
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Presentación ------------------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Tutor del módulo}{Aprendizaje profundo}
        \begin{columns}
            % Columna izquierda
            \begin{column}{0.7\textwidth}
                \textbf{Rodolfo Ferro} (\href{mailto:ferro@cimat.mx}{ferro@cimat.mx})
                {\scriptsize \begin{itemize}
                    \item Sr. SWE (Data Engineer) @ Bisonic México
                    \item Miembro del Consejo Consultivo para el Desarrollo Económico, Creatividad e Innovación de León
                    \item \textbf{Formación:} BMath, CSysEng, StatMethodsSpc (\textit{ongoing})
                    \item \textbf{Experiencia:} ML Engineer @ Vindoo.ai (España), Sherpa Digital en IA @ Microsoft México, AI Research Assistant @ CIMAT \& AI Research Intern @ Harvard.
                \end{itemize}}
            \end{column}
            % Columna derecha
            \begin{column}{0.25\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[width=1\textwidth]{rodo.png}
                \end{figure}
            \end{column}
        \end{columns}
    \end{frame}
	
	\sepframe[title={Tabla de contenidos}]
	
    \frame{\tableofcontents}

	
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Intro al DL -------------------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Intro al aprendizaje profundo}
    
    % ----------- Motivación 01 ------------------
    \subsection{Motivación}
    
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-01}
        \end{figure}
    \end{frame}
    
    % ----------- Motivación 02 ------------------
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-02}
        \end{figure}
    \end{frame}
    
    % ----------- Motivación 03 ------------------
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-03}
        \end{figure}
    \end{frame}

    % ----------- Motivación 04 ------------------
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-04}
        \end{figure}
    \end{frame}
    
    % ----------- ¿Qué es el DL? 01 --------------
    \subsection{Introducción}
    
    \begin{frame}{¿Qué es el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{quotation}[David Foster (Generative Deep Learning)]
            El aprendizaje profundo (Deep Learning) comprende algoritmos de Machine Learning que (particularmente) utilizan múltiples capas apiladas de unidades de procesamiento para aprender representaciones en un alto nivel sobre datos no estructurados.
        \end{quotation}
    \end{frame}
    
    % ----------- ¿Qué es el DL? 02 --------------
    \begin{frame}{¿Qué es el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{ai-ml-dl}
        \end{figure}
    \end{frame}
    
    % ----------- ¿Qué es el DL? 03 --------------
    \begin{frame}{¿Qué es el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{itemize}
                \item \textbf{Inteligencia artificial:} Cualquier técnica que permita
                a las computadoras emular o imitar el comportamiento humano.
                \item \textbf{Aprendizaje de máquina:} Capacidad de aprender sin ser programado explícitamente, enfoque en los algoritmos y la matemtica.
                \item \textbf{Aprendizaje profundo:} Extrae patrones de datos utilizando redes neuronales.
            \end{itemize}
        \end{center}
    \end{frame}
    
    % ----------- ¿Por qué el DL? 01 ------------
    \begin{frame}{¿Por qué el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{ml-01}
        \end{figure}
    \end{frame}

    % ----------- ¿Por qué el DL? 02 ------------
    \begin{frame}{¿Por qué el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{ml-02}
        \end{figure}
    \end{frame}
    
    % ----------- ¿Por qué el DL? 02 ------------
    \begin{frame}{¿Por qué el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{itemize}
                \item La ingeniería de características requiere mucho tiempo, es suceptible a errores y no es escalable consistentemente con datos complejos. Mejor busquemos aprender las características subyacentes directamente de los datos.
                \item Las redes neuronales artificales existen desde hace décadas, pero su predominio actual reside principalmente en los siguientes tres aspectos:
                \begin{enumerate}
                    \item Hardware (GPUs, etc. + Paralelización)
                    \item Software (Frameworks para trabajar con NNs)
                    \item Grandes cantidades de datos
                \end{enumerate}
                \item El aprendizaje profundo está revolucionando muchos campos.
            \end{itemize}
        \end{center}
    \end{frame}
    
    % ----------- Contexto histórico ------------
    \subsection{Contexto histórico}
    
    \begin{frame}{Contexto histórico}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{santiago}
            \caption{Santiago Ramón y Cajal}
        \end{figure}
    \end{frame}
    
    % ----------- Santiago Ramón y Cajal --------
    \begin{frame}{Contexto histórico}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.3\textwidth]{santiago-neuron} $\longrightarrow$
            \includegraphics[width=0.5\textwidth]{neuron}
            
            \let\thefootnote\relax\footnote{{\tiny “Santiago Ramón y Cajal Drawings.” Janelia Research Campus. Accessed July 5, 2024. \href{https://www.janelia.org/archive/santiago-ramón-y-cajal-drawings}{https://www.janelia.org}}}
            
            \let\thefootnote\relax\footnote{{\tiny “Overview of Neuron Structure and Function (Article).” Khan Academy. Accessed July 5, 2024. \href{https://www.khanacademy.org/science/biology/human-biology/neuron-nervous-system/a/overview-of-neuron-structure-and-function}{https://www.khanacademy.org}}}
        \end{figure}
    \end{frame}
    
    % ----------- McCulloch & Pitts -------------
    \begin{frame}{TLU}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{mcculloch-pitts}
            \caption{Warren McCulloch \& Walter Pitts}
        \end{figure}
    \end{frame}
    
    % ----------- TLU ---------------------------
    \begin{frame}{TLU}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$H$};
                \node[output](o) at (5,1){$y$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
                
                \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (-2,3) -- (-0.75,3) node [black,midway,yshift=+0.6cm]{entrada};
                \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (-0.25,3) -- (1,3) node [black,midway,yshift=+0.6cm]{pesos};
                \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (4.5,1.5) -- (5.75,1.5) node [black,midway,yshift=+0.6cm]{salida};
                
                \draw[->] (1,-1.75)node[sum]{$\Sigma$} -- (3,-1.75)[xshift=30]node{$\displaystyle\sum_{i=1}^{n}w_ix_i$};
            \end{tikzpicture}
        \end{center}
    \end{frame}
    
    % -------- Bias y función de activación 01 --
    \begin{frame}{Bias y función de activación}{Intro al aprendizaje profundo}
        La operación matemática que realiza la neurona para la decisión de umbralización se puede escribir como:
        
        $$ f(\textbf{x}) =
        \begin{cases}
            0 & \text{si $\displaystyle\sum_{i}w_ix_i <$ umbral o threshold} \\
            1 & \text{si $\displaystyle\sum_{i}w_ix_i \geq$ umbral o threshold} \\
        \end{cases}$$
        
        donde $i \in \{1, 2, ..., n\}$, y así, $\textbf{x} = (x_1, x_2, ..., x_n)$.
    \end{frame}
    
    % -------- Bias y función de activación 01 --
    \begin{frame}{Bias y función de activación}{Intro al aprendizaje profundo}
        De lo anterior, podemos despejar el umbral y escribirlo como $b$, obteniendo:
        
        $$ f(\textbf{x}) =
        \begin{cases}
            0 & \text{si $\displaystyle\sum_{i}w_ix_i + b < 0$} \\
            1 & \text{si $\displaystyle\sum_{i}w_ix_i + b > 0$} \\
        \end{cases}$$
        
        donde $\textbf{x} = (x_1, x_2, ..., x_n)$ y $i \in \{1, 2, ..., n\}$.

        A esto que escribimos como $b$, también se le conoce como \textbf{bias}, y una interpretación es describir \textit{qué tan susceptible es la neurona a \textbf{dispararse}} (como se exploró en el ejemplo práctico de la identificación de la actividad de Julieta).
    \end{frame}
    
    % -------- Bias y función de activación 03 --
    \begin{frame}{Bias y función de activación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.425\textwidth]{heavyside} $\longrightarrow$
            \includegraphics[width=0.45\textwidth]{sigmoid-plot}
        \end{figure}
    \end{frame}
    
    % ----------- El Perceptrón 01 --------------
    \subsection{Perceptrón}
    
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{rosenblatt}
            \caption{Frank Rosenblatt}
        \end{figure}
    \end{frame}
    
    
    % ----------- El Perceptrón 02 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$\sigma$};
                \node[output](o) at (5,1){$y$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            
            \vspace{20pt}
            ¡Y se agrega un algoritmo formal de entrenamiento!\\(\textit{Backpropagation})
        \end{center}
    \end{frame}
    
    % ----------- Idea de entrenamiento 01 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{training-idea-01}
        \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 02 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{training-idea-02}
    \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 03 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{training-idea-03}
    \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 04 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{training-idea-04}
        \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 05 ------
    \begin{frame}{Medición del error}{Intro al aprendizaje profundo}
        Dado el vector $X$, \textbf{¿qué vector ($A, B, C$) se le \textit{parece} más?}
        
        \begin{align*}
            X &= \begin{bmatrix}
                0.5 \\
                0.3 \\
                0.7
            \end{bmatrix}
        \end{align*}
        
        \begin{align*}
            A = \begin{bmatrix}
                0.3 \\
                0.3 \\
                0.3
            \end{bmatrix},
            B = \begin{bmatrix}
                0.6 \\
                0.2 \\
                0.6
            \end{bmatrix},
            C = \begin{bmatrix}
                -0.5 \\
                -0.3 \\
                -0.7
            \end{bmatrix}
        \end{align*}
    \end{frame}
    
    % ----------- Idea de entrenamiento 05 ------
    \begin{frame}{Medición del error}{Intro al aprendizaje profundo}
        Dado el vector $X$, \textbf{¿qué vector ($A, B, C$) se le \textit{parece} más?}
        
        \begin{align*}
            X &= \begin{bmatrix}
                0.5 \\
                0.3 \\
                0.7
            \end{bmatrix}
        \end{align*}
        
        \begin{align*}
            A = \begin{bmatrix}
                0.3 \\
                0.3 \\
                0.3
            \end{bmatrix},
            \colorbox{green!20}{$B = \begin{bmatrix}
                0.6 \\
                0.2 \\
                0.6
            \end{bmatrix}$},
            C = \begin{bmatrix}
                -0.5 \\
                -0.3 \\
                -0.7
            \end{bmatrix}
        \end{align*}
    \end{frame}
    
    % ----------- Idea de entrenamiento 06 ------
    \begin{frame}{Optimización del error}{Intro al aprendizaje profundo}
        \begin{columns}
            % Columna izquierda
            \begin{column}{0.6\textwidth}
                \begin{itemize}
                    \item \textbf{Error:} Es una función.
                    \item \textbf{Optimizar:} Maximizar o minimizar.
                    \item \textbf{Gradiente:} Derivada de una función vectorial, proporciona información sobre máximos o mínimos.
                    \item \textbf{Descenso de gradiente:} Algoritmo para, iterativamente, buscar optimizar una función.
                    \item \textbf{Limitantes:} 
                        \begin{itemize}
                            \item Max's/min's locales.
                            \item Tamaño de salto en gradiente
                        \end{itemize}
                \end{itemize}
            \end{column}
            % Columna derecha
            \begin{column}{0.35\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[width=1\textwidth]{gd}
                \end{figure}
            \end{column}
        \end{columns}
    \end{frame}
    
    % ----------- Observaciones 01 ----------------
    \begin{frame}{Observaciones}{Intro al aprendizaje profundo}
        Hasta este punto, debemos notar que hay algunas observaciones importantes:
        \begin{itemize}
            \item \textbf{TLUs:} 
            \begin{itemize}
                \item No existe un algoritmo de aprendizaje formal → Búsqueda de pesos.
                \item Se limita a propagación hacia adelante (\textit{forward pass/forward propagation})
            \end{itemize}
            \item \textbf{Perceptrón:} Puede utilizar retropropagación, introducido en 1958.
            \item \textbf{Retropropagación:} Algoritmo para realizar ajustes en los valores de los pesos.
            \item \textbf{Limitantes:} Separabilidad lineal.
            \item \textbf{¿Alguna otra observación?}
        \end{itemize}
    \end{frame}
    
    % ----------- Ejercicio 01 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Manzanas vs. Naranjas}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.4\textwidth]{apple-orange}
        \end{figure}
    \end{frame}
    
    % ----------- Lecturas recomendadas 01 ------
    \begin{frame}{Lecturas recomendadas}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Breve historia sobre \colorbox{blue!10}{\href{https://telefonicatech.com/blog/historia-de-la-ia-frank-rosenblatt-y-e}{el perceptrón}}
            \item Post sobre \colorbox{blue!10}{\href{https://lamaquinaoraculo.com/deep-learning/el-perceptron-de-rosenblatt/}{el perceptrón de Rosenblatt}}
            \item Post sobre la \colorbox{blue!10}{\href{https://lamaquinaoraculo.com/deep-learning/la-funcion-de-activacion/}{función de activación}}
            \item \colorbox{blue!10}{\href{https://ploomber.io/blog/threshold/}{Selección de threshold}} para clasificadores binarios
            \item Post sobre \colorbox{blue!10}{\href{https://www.ibm.com/mx-es/topics/neural-networks}{redes neuronales}} por IBM
        \end{itemize}
    \end{frame}
    
    % ----------- Producto matricial 01 ---------
    \subsection{Perceptrón multicapa}
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
        -2 & 1 & 0\\
        -2 & 2 & 1\\
        0 & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
         &  & \\
         &  & \\
         &  & 
        \end{bmatrix}$$
    \end{frame}
    
    % ----------- Producto matricial 02 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            \colorbox{blue!10}{2} & \colorbox{blue!10}{5} & \colorbox{blue!10}{2}\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            \colorbox{blue!10}{-2} & 1 & 0\\
            \colorbox{blue!10}{-2} & 2 & 1\\
            \colorbox{blue!10}{0} & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
           \colorbox{blue!10}{14} &  & \\
            &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(2 \cdot -2) + (5 \cdot -2) + (2 \cdot 0) = -14$$
    \end{frame}
    
    % ----------- Producto matricial 03 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            \colorbox{blue!10}{2} & \colorbox{blue!10}{5} & \colorbox{blue!10}{2}\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            -2 & \colorbox{blue!10}{1} & 0\\
            -2 & \colorbox{blue!10}{2} & 1\\
            0 & \colorbox{blue!10}{0} & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & \colorbox{blue!10}{12} & \\
            &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(2 \cdot 1) + (5 \cdot 2) + (2 \cdot 0) = 12$$
    \end{frame}
    
    % ----------- Producto matricial 04 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            \colorbox{blue!10}{2} & \colorbox{blue!10}{5} & \colorbox{blue!10}{2}\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
        -2 & 1 & \colorbox{blue!10}{0}\\
        -2 & 2 & \colorbox{blue!10}{1}\\
        0 & 0 & \colorbox{blue!10}{3}
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & \colorbox{blue!10}{11}\\
            &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(2 \cdot 0) + (5 \cdot 1) + (2 \cdot 3) = 11$$
    \end{frame}
    
    % ----------- Producto matricial 05 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            \colorbox{blue!10}{1} & \colorbox{blue!10}{0} & \colorbox{blue!10}{-2}\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            \colorbox{blue!10}{-2} & 1 & 0\\
            \colorbox{blue!10}{-2} & 2 & 1\\
            \colorbox{blue!10}{0} & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & 11\\
            \colorbox{blue!10}{-2} &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(1 \cdot -2) + (0 \cdot -2) + (-2 \cdot 0) = -2$$
    \end{frame}
    
    % ----------- Producto matricial 06 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            \colorbox{blue!10}{1} & \colorbox{blue!10}{0} & \colorbox{blue!10}{-2}\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            \colorbox{blue!10}{-2} & 1 & 0\\
            \colorbox{blue!10}{-2} & 2 & 1\\
            \colorbox{blue!10}{0} & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & 11\\
            \colorbox{blue!10}{-2} &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$\cdots$$
    \end{frame}
    
    % ----------- Producto matricial 07 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            -2 & 1 & 0\\
            -2 & 2 & 1\\
            0 & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & 11\\
            -2 & 1 & -6\\
            -8 & 5 & 4
        \end{bmatrix}$$
    \end{frame}
    
    % ----------- El Perceptrón 03 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$f$};
                \node[output](o) at (5,1){$\hat{y}$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            \vspace{10pt}
            $$\hat{y} = f\left(\displaystyle\sum_{i}^{n}w_ix_i + b\right)$$
        \end{center}
    \end{frame}
    
    % ----------- El Perceptrón 04 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$f$};
                \node[output](o) at (5,1){$\hat{y}$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            \vspace{10pt}
            $$\displaystyle\sum_{i}^{n}w_ix_i = w_1x_1 + w_2x_2 + \cdots + w_nx_n$$
        \end{center}
    \end{frame}
    
    % ----------- El Perceptrón 05 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$f$};
                \node[output](o) at (5,1){$\hat{y}$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            \vspace{10pt}
            $$\mathbf{w^{T}}\mathbf{x} = [w_1 w_2 \cdots w_n] \begin{bmatrix}
                x_1 \\
                x_2 \\
                \vdots \\
                x_n \\
            \end{bmatrix} = w_1x_1 + w_2x_2 + \cdots + w_nx_n$$
        \end{center}
    \end{frame}
    
    % ----------- Producto matricial 08 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-01}
        \end{figure}
    \end{frame}
    
    % ----------- Producto matricial 09 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-02}
        \end{figure}
    \end{frame}
    
    % --------- Composición de funciones 01 -----
    \begin{frame}{Composición de funciones}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-03}
        \end{figure}
    \end{frame}
    
    % --------- Composición de funciones 02 -----
    \begin{frame}{Composición de funciones}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-04}
        \end{figure}
    \end{frame}

    % --------- Funciones de activación ---------
    \begin{frame}{Funciones de activación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{activation-functions}
            $$f(x) = \sigma(x) = \dfrac{1}{1 + e^{-x}} \Rightarrow f'(x) = f(x)(1 - f(x))$$
            $$f(x) = tanh(x) = \dfrac{e^x - e^{-x}}{e^x + e^{-x}} \Rightarrow f'(x) = 1 - f(x)^2$$
        \end{figure}
    \end{frame}
    
    % ----------- El Perceptrón Multicapa 01 ----
    \begin{frame}{El perceptrón multicapa}{Intro al aprendizaje profundo}
        \colorlet{myred}{red!80!black}
        \colorlet{myblue}{blue!80!black}
        \colorlet{mygreen}{green!60!black}
        \colorlet{mydarkred}{myred!40!black}
        \colorlet{mydarkblue}{myblue!40!black}
        \colorlet{mydarkgreen}{mygreen!40!black}
        \tikzstyle{node}=[very thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
        \tikzstyle{connect}=[->,thick,mydarkblue,shorten >=1]
        \tikzset{ % node styles, numbered for easy mapping with \nstyle
            node 1/.style={node,mydarkgreen,draw=mygreen,fill=mygreen!25},
            node 2/.style={node,mydarkblue,draw=myblue,fill=myblue!20},
            node 3/.style={node,mydarkred,draw=myred,fill=myred!20},
        }
        \def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3
        
        \begin{center}
            \begin{tikzpicture}[x=2.4cm,y=1	.2cm]
                \readlist\Nnod{4,3,2} % array of number of nodes per layer
                \readlist\Nstr{n,m,k} % array of string number of nodes per layer
                \readlist\Cstr{x,h^{(\prev)},y} % array of coefficient symbol per layer
                \def\yshift{0.55} % shift last node for dots
                
                % LOOP over LAYERS
                \foreachitem \N \in \Nnod{
                    \def\lay{\Ncnt} % alias of index of current layer
                    \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
                    \foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
                        \x=\lay; \n=\nstyle;
                        \index=(\i<\N?int(\i):"\Nstr[\n]");}] in {1,...,\N}{ % loop over nodes
                        % NODES
                        \node[node \n] (N\lay-\i) at (\x,\y) {$\strut\Cstr[\n]_{\index}$};
                        
                        % CONNECTIONS
                        \ifnumcomp{\lay}{>}{1}{ % connect to previous layer
                            \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                                \draw[white,line width=1.2,shorten >=1] (N\prev-\j) -- (N\lay-\i);
                                \draw[connect] (N\prev-\j) -- (N\lay-\i);
                            }
                            \ifnum \lay=\Nnodlen
                            \draw[connect] (N\lay-\i) --++ (0.5,0); % arrows out
                            \fi
                        }{
                            \draw[connect] (0.5,\y) -- (N\lay-\i); % arrows in
                        }
                        
                    }
                    \path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.6] {$\vdots$}; % dots
                }
                
                % LABELS
                \node[above=0.25,align=center,mydarkgreen] at (N1-1.90) {Capa de \\[-0.2em]entrada};
                \node[above=0.25,align=center,mydarkblue] at (N2-1.90) {Capas\\[-0.2em]ocultas};
                \node[above=0.25,align=center,mydarkred] at (N\Nnodlen-1.90) {Capa de\\[-0.2em]salida};
                
            \end{tikzpicture}
        \end{center}
    \end{frame}
    
    % ----------- El Perceptrón Multicapa 01 ----
    \begin{frame}{El perceptrón multicapa}{Intro al aprendizaje profundo}
        \tikzstyle{node}=[very thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
        \tikzstyle{connect}=[->,thick,mydarkblue,shorten >=1]
        \tikzset{ % node styles, numbered for easy mapping with \nstyle
            node 1/.style={node,mydarkgreen,draw=mygreen,fill=mygreen!25},
            node 2/.style={node,mydarkblue,draw=myblue,fill=myblue!20},
            node 3/.style={node,mydarkred,draw=myred,fill=myred!20},
        }
        \def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3
        \begin{center}
            \begin{tikzpicture}[x=2.4cm,y=1	.2cm]
                \readlist\Nnod{3,3,1} % array of number of nodes per layer
                \readlist\Nstr{n,m,1} % array of string number of nodes per layer
                \readlist\Cstr{x,h^{(\prev)},y} % array of coefficient symbol per layer
                \def\yshift{0.55} % shift last node for dots
                
                % LOOP over LAYERS
                \foreachitem \N \in \Nnod{
                    \def\lay{\Ncnt} % alias of index of current layer
                    \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
                    \foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
                        \x=\lay; \n=\nstyle;
                        \index=(\i<\N?int(\i):"\Nstr[\n]");}] in {1,...,\N}{ % loop over nodes
                        % NODES
                        \node[node \n] (N\lay-\i) at (\x,\y) {$\strut\Cstr[\n]_{\index}$};
                        
                        % CONNECTIONS
                        \ifnumcomp{\lay}{>}{1}{ % connect to previous layer
                            \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                                \draw[white,line width=1.2,shorten >=1] (N\prev-\j) -- (N\lay-\i);
                                \draw[connect] (N\prev-\j) -- (N\lay-\i);
                            }
                            \ifnum \lay=\Nnodlen
                            \draw[connect] (N\lay-\i) --++ (0.5,0); % arrows out
                            \fi
                        }{
                            \draw[connect] (0.5,\y) -- (N\lay-\i); % arrows in
                        }
                        
                    }
                    \path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.6] {$\vdots$}; % dots
                }
                
                % LABELS
                \node[above=0.25,align=center,mydarkgreen] at (N1-1.90) {Capa de \\[-0.2em]entrada};
                \node[above=0.25,align=center,mydarkblue] at (N2-1.90) {Capas\\[-0.2em]ocultas};
                \node[above=0.25,align=center,mydarkred] at (N\Nnodlen-1.90) {Capa de\\[-0.2em]salida};
            \end{tikzpicture}
            \vspace{10pt}\\
            Para $\mathbf{x}^{(i)}=(x^{(i)}_1, x^{(i)}_2, \dots, x^{(i)}_n)$, $y^{(i)}$, tendríamos que la salida es $\hat{y}^{(i)}_1 = f(x^{(i)})$. 
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 01 ----------------
    \subsection{Aprendizaje}
    \begin{frame}{Cuantificación del error}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item La \textbf{función de pérdida (\textit{loss})} de nuestra red neuronal \textit{mide} el \textit{costo} asociado a predicciones incorrectas.
            \item Si observaciones (de entrada y salida) $(x^{(i)}, y^{(i)})$ y consideramos a la salida como función de $x^{(i)}$ y $\mathbf{W}$, entonces las salidas son $\hat{y} = f(x^{(i)}; \mathbf{W})$ y la función de pérdida puede escribirse como:
            $$\mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})$$
            Es decir, una función que mida la salida \textit{real} con la \textit{predicción}.
            \underline{Todo esto para una observación $i$.}
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 02 ----------------
    \begin{frame}{Cuantificación del error}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Para todas las observaciones:
            $$\mathbf{J(W)} = \displaystyle \dfrac{1}{n}\sum_{i=1}^n \mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})$$
            A esta función se le conoce como función de costo o función objetivo (lo que queremos minimizar).
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 03 ----------------
    \begin{frame}{Algunos ejemplos}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item \textit{\textbf{Binary Cross Entropy Loss}}:
            Se puede utilizar con modelos que devuelven como salida una probabilidad entre 0 y 1.
            $$\mathbf{J(W)} = \displaystyle \dfrac{1}{n}\sum_{i=1}^n y^{(i)} \log(f(x^{(i)}; \mathbf{W})) + (1 - y^{(i)}) \log(1 - f(x^{(i)}; \mathbf{W}))$$
            \item \textit{\textbf{Mean Squared Error (MSE) Loss}}:
            Se puede utilizar con modelos de regresión que generan números reales continuos.
            $$\mathbf{J(W)} = \displaystyle \dfrac{1}{n}\sum_{i=1}^n \left( y^{(i)} - f(x^{(i)}; \mathbf{W})\right)^2$$
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 03 ----------------
    \begin{frame}{Optimización del error}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Queremos encontrar los pesos ideales de la red neuronal, los cuales minimizan $\mathbf{J(W)}$, es decir:
            $$\mathbf{W^*} = \displaystyle \operatorname*{argmin}_\mathbf{W} \dfrac{1}{n}\sum_{i=1}^n \mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})$$
            $$\mathbf{W^*} = \displaystyle \operatorname*{argmin}_\mathbf{W} \mathbf{J(W)}$$
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 04 ----------------
    \begin{frame}{Descenso de gradiente}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Calcular el gradiente $\dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
    \end{frame}
    
    % ----------- Aprendizaje 05 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{$w_1$};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{15pt}
            
            ¿Cómo se calculan los gradientes?\\
            Con la regla de la cadena.
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 06 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{$w_1$};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{\colorbox{yellow!20}{$w_2$}};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{10pt}
            
            $$\dfrac{\partial \mathbf{J(W)}}{\partial \colorbox{yellow!20}{$w_2$} } = \dfrac{\partial \mathbf{J(W)}}{\partial \hat{y}} \cdot \dfrac{\partial \hat{y}}{\partial \colorbox{yellow!20}{$w_2$}}$$
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 07 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{\colorbox{yellow!20}{$w_1$}};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{10pt}
            
            $$\dfrac{\partial \mathbf{J(W)}}{\partial \colorbox{yellow!20}{$w_1$} } = \dfrac{\partial \mathbf{J(W)}}{\partial \hat{y}} \cdot \dfrac{\partial \hat{y}}{\partial \colorbox{yellow!20}{$w_1$}}$$
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 08 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{\colorbox{yellow!20}{$w_1$}};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{10pt}
            
            $$\dfrac{\partial \mathbf{J(W)}}{\partial \colorbox{yellow!20}{$w_1$} } = \dfrac{\partial \mathbf{J(W)}}{\partial \hat{y}} \cdot \dfrac{\partial \hat{y}}{\partial z_1 } \cdot \dfrac{\partial z_1}{\partial \colorbox{yellow!20}{$w_1$}}$$
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 09 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{$w_1$};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{15pt}
            
            ¿Cómo se calculan los gradientes?\\
            Con la regla de la cadena.\\
            
            Esto se repite para \textbf{cada peso} en la red neuronal, usando los gradientes de las capas posteriores.
        \end{center}
    \end{frame}
    
    % --------- Learning rate -------------------
    \begin{frame}{Learning rate}{Intro al aprendizaje profundo}
        \begin{center}
            La actualización de pesos está dada por:
            $$\mathbf{W} \leftarrow \mathbf{W} - \colorbox{yellow!20}{$\eta$} \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$$
            \begin{figure}
                \centering
                \includegraphics[width=0.8\textwidth]{lr}
            \end{figure}
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 10 ----------------
    \begin{frame}{Descenso de gradiente}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Calcular el gradiente \colorbox{yellow!20}{$\dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}^*$}
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
        \begin{itemize}
            \item $^*$Esto es muy pesado de calcular (computacionalmente).
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 11 ----------------
    \begin{frame}{Descenso de gradiente estocástico}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente estocástico}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Seleccionar observación $i$
                \item \hspace*{8pt} Calcular el gradiente \colorbox{yellow!20}{$\dfrac{\partial \mathbf{J_i(W)}}{\partial \mathbf{W}}^*$}
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
        \begin{itemize}
            \item $^*$Esto es muy sencillo de calcular (computacionalmente), pero es estocástico.
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 12 ----------------
    \begin{frame}{Descenso de gradiente estocástico}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente estocástico - \textit{Mini batches}}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Seleccionar un batch $B$ de observaciones
                \item \hspace*{8pt} Calcular el gradiente \colorbox{yellow!20}{$\dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}} = \frac{1}{B} \sum_{k=1}^B \dfrac{\partial \mathbf{J_k(W)}}{\partial \mathbf{W}}^*$}
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
        \begin{itemize}
            \item $^*$Esto es rápido de calcular (computacionalmente), y da una mejor estimación del gradiente.
        \end{itemize}
    \end{frame}
    
    % ----------- Observaciones 02 ----------------
    \begin{frame}{Observaciones}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Los \textbf{frameworks} para aprendizaje profundo (como TensorFlow, PyTorch, etc.) ya hacen la diferenciación y optimización por nosotros, es decir, ya calculan el gradiente y actualizan los pesos.
            \item Nosotros exploraremos el uso de \textbf{TensorFlow} a través de su API de alto nivel, \textbf{Keras}, para las redes neuronales que estaremos construyendo.
            \item Comenzaremos retomando algunos de los problemas planteados en la sesión anterior.
        \end{itemize}
    \end{frame}
    
    % ----------- TensorFlow --------------------
    \begin{frame}{TensorFlow}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{figure}
                \centering
                \includegraphics[width=0.5\textwidth]{tf}
            \end{figure}
            \colorbox{blue!10}{\href{https://www.tensorflow.org/}{TensorFlow}} es un framework open-source para Machine Learning desarrollado por Google. Utilizado para construir y entrenar redes neuronales artificiales.
        \end{center}
    \end{frame}
    
    % ----------- Ejercicio 02 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Exploración del TensorFlow Playground}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.65\textwidth]{tf-playground}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 03 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Problema de separabilidad lineal}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{xor}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 04 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Exploración con TensorFlow}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{tf}
        \end{figure}
    \end{frame}
    
    % ----------- Lecturas recomendadas 02 ------
    \begin{frame}{Lecturas recomendadas}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Setting the   \colorbox{blue!10}{\href{https://www.jeremyjordan.me/nn-learning-rate/}{learning rate}} of your neural network
            \item \colorbox{blue!10}{\href{https://hmkcode.com/ai/backpropagation-step-by-step/}{Retropropagación}} paso a paso
            \item \colorbox{blue!10}{\href{https://www.tensorflow.org/tutorials}{TensorFlow Tutorials}}
            \item Libro \colorbox{blue!10}{\href{http://neuralnetworksanddeeplearning.com/}{Neural Networks and Deep Learning}}
        \end{itemize}
    \end{frame}
    
    % ----------- Regularización 01 --------------
    \subsection{Regularización}
    
    \begin{frame}{El problema del overfitting}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.85\textwidth]{overfitting}
        \end{figure}
    \end{frame}
    
    % ----------- Regularización 02 --------------
    \begin{frame}{Regularización}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item La \textbf{regularización} consiste en alguna técnica que sirve para evitar que un modelo se sobreajuste.
            \item Es necesaria porque ayuda a mejorar la generalización de nuestro modelo con datos no vistos.
            \item Los métodos de regularización que exploraremos serán:
            \begin{itemize}
                \item \textit{Dropout}
                \item \textit{Early stopping}
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Dropout 01 ---------------------
    \begin{frame}{Dropout}{Intro al aprendizaje profundo}
        \def\layersep{1}
        \def\nodesep{0.75}
        
        \begin{tikzpicture}[
            node/.style={circle, draw, thick},
            ]
            
            \foreach \y in {1,...,5}{
                \node[node] (i\y) at (0,\nodesep*\y) {};
                \node[node, right=\layersep of i\y] (h1\y) {};
                \node[node, right=\layersep of h1\y] (h2\y) {};
            }
            
            \node[node, right=\layersep of h22] (o1) {};
            \node[node, right=\layersep of h24] (o2) {};
            
            \foreach \source in {1,...,5}
            \foreach \dest in {1,...,5}{
                \path[-stealth, thick] (i\source) edge (h1\dest);
                \path[-stealth, thick] (h1\source) edge (h2\dest);
            }
            \foreach \source in {1,...,5}
            \foreach \dest in {1,2}
            \draw[-stealth, thick] (h2\source) -- (o\dest);
            
            \draw[-stealth, thick] (4.2,3*\nodesep) -- node[above,font=\bfseries]{dropout} (6, 3*\nodesep);
            
            % Boundary
            
            \foreach \y in {1,...,5}
            \node[node, right=10em of h2\y] (di\y) {};
            
            \node[red,font=\huge] at (di1) {$\times$};
            \node[red,font=\huge] at (di3) {$\times$};
            
            \foreach \y in {1,...,5}
            \node[node, right=\layersep of di\y] (dh1\y) {};
            
            \node[red,font=\huge] at (dh11) {$\times$};
            \node[red,font=\huge] at (dh13) {$\times$};
            \node[red,font=\huge] at (dh14) {$\times$};
            
            \foreach \y in {1,...,5}
            \node[node, right=\layersep of dh1\y] (dh2\y) {};
            
            \node[red,font=\huge] at (dh22) {$\times$};
            \node[red,font=\huge] at (dh24) {$\times$};
            
            \node[node, right=\layersep of dh22] (do1) {};
            \node[node, right=\layersep of dh24] (do2) {};
            
            \foreach \source in {2,4,5}
            \foreach \dest in {2,5}
            \draw[-stealth, thick] (di\source) -- (dh1\dest);
            
            \foreach \source in {2,5}
            \foreach \dest in {1,3,5}
            \draw[-stealth, thick] (dh1\source) -- (dh2\dest);
            
            \foreach \source in {1,3,5}
            \foreach \dest in {1,2}
            \draw[-stealth, thick] (dh2\source) -- (do\dest);
            
        \end{tikzpicture}
    \end{frame}
    
    % ----------- Dropout 02 ---------------------
    \begin{frame}{Dropout}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Durante el entrenamiento, establecemos aleatoriamente algunas activaciones en 0
            \begin{itemize}
                \item Típicamente hacemos "drop" del 50\% de activaciones en una capa.
                \item Esto forza a la red a no depender de ningún nodo/neurona.
            \end{itemize}
            \item Podemos realizar el dropout en TensorFlow utilizando la capa \texttt{tf.keras.layers.Dropout(0.5)}, donde el 0.5 puede variar de acuerdo a lo especificado.
        \end{itemize}
    \end{frame}
    
    % ----------- Early stopping 01 --------------
    \begin{frame}{Early stopping}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.45\textwidth]{early-stop-01}
            \includegraphics[width=0.45\textwidth]{early-stop-02}
        \end{figure}
    \end{frame}
    
    % ----------- Early stopping 01 --------------
    \begin{frame}{Early stopping}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item El \textit{Early Stopping} puede ser realizado en TensorFlow de manera sencilla creando un callback (función que se llama en cada iteración durante el entrenamiento de la red neuronal):\\
        \end{itemize}
        
        \texttt{\\model = tf.keras.models.Sequential(...)}\\
        \texttt{callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)}\\
        \texttt{history = model.fit(..., callbacks=[callback])}
    \end{frame}
    
    % Mover/eliminar conforme se agregan los contenidos
    \subsection{Regularización}
    \section{Visión computacional profunda}
    \section{Modelado profundo de secuencias}
    \section{Modelado generativo profundo}
    \section{Panorama actual y futuro}
    
    % ----------- Lecturas recomendadas 03 ------
    \begin{frame}{Lecturas recomendadas}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item \colorbox{blue!10}{\href{https://jmlr.org/papers/v15/srivastava14a.html}{Artículo}} "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
            \item \colorbox{blue!10}{\href{https://www.pinecone.io/learn/regularization-in-neural-networks/}{Regularización en Redes Neuronales}}
            \item \colorbox{blue!10}{\href{https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing}{Vanishing and Exploding Gradients in Neural Network Models:}} Debugging, Monitoring, and Fixing
        \end{itemize}
    \end{frame}

	
\end{document}

%%% Local Variables:
%%% mode: lualatex
%%% TeX-master: Rodolfo Ferro
%%% End:
