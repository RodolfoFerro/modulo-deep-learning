\documentclass[10pt,border=3pt,tikz]{beamer}

\usepackage{pgfpages}
\usepackage[T1]{fontenc}

\usepackage{pdfpages}

\usepackage{tikz}
\usetikzlibrary{matrix, positioning}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{etoolbox}
\usepackage{listofitems} % for \readlist to create arrays
\tikzstyle{mynode}=[thick,draw=blue,fill=blue!20,circle,minimum size=15]
%\setbeameroption{show notes on second screen }
\usetheme[
% nojauge,
% nomail,
% rule,
delaunay,
amurmapleblack
]{Amurmaple}

\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{emoji}
\graphicspath{ {./images/} }

\usepackage{xcolor}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}
\tikzset{
    >=latex, % for default LaTeX arrow head
    node/.style={thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6},
    node in/.style={node,green!20!black,draw=mygreen!30!black,fill=mygreen!25},
    node hidden/.style={node,blue!20!black,draw=myblue!30!black,fill=myblue!20},
    node convol/.style={node,orange!20!black,draw=myorange!30!black,fill=myorange!20},
    node out/.style={node,red!20!black,draw=myred!30!black,fill=myred!20},
    connect/.style={thick,mydarkblue}, %,line cap=round
    connect arrow/.style={-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1},
    node 1/.style={node in}, % node styles, numbered for easy mapping with \nstyle
    node 2/.style={node hidden},
    node 3/.style={node out}
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3

\definecolor{newremark}{rgb}{0.7,0.2,0.2}
\colorlet{AmurmapleRemarkColor}{newremark}

\title[Deep Learning]{Aprendizaje profundo}
\author[R.~Ferro (@rodo\_ferro)]{Rodolfo Ferro}
\subtitle{Módulo 5}
\institute[ENES Unidad León]{Diplomado en Ciencia de Datos\\
	Escuela Nacional de Estudios Superiores, Unidad León}
\date{Agosto, 2024}
\titlegraphic{\includegraphics[width=3cm]{logo.png}}
\mail{ferro@cimat.mx}
\webpage{https://rodolfoferro.xyz}
% \collaboration{in collaboration with \LaTeX{}}
\logo{\includegraphics[width=1.6cm]{logo.png}}

\begin{document}
    
	\maketitle
    
        
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Presentación ------------------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{frame}{Tutor del módulo}{Aprendizaje profundo}
        \begin{columns}
            % Columna izquierda
            \begin{column}{0.7\textwidth}
                \textbf{Rodolfo Ferro} (\href{mailto:ferro@cimat.mx}{ferro@cimat.mx})
                {\scriptsize \begin{itemize}
                    \item Sr. SWE (Data Engineer) @ Bisonic México
                    \item Miembro del Consejo Consultivo para el Desarrollo Económico, Creatividad e Innovación de León
                    \item \textbf{Formación:} BMath, CSysEng, StatMethodsSpc (\textit{ongoing})
                    \item \textbf{Experiencia:} ML Engineer @ Vindoo.ai (España), Sherpa Digital en IA @ Microsoft México, AI Research Assistant @ CIMAT \& AI Research Intern @ Harvard.
                \end{itemize}}
            \end{column}
            % Columna derecha
            \begin{column}{0.25\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[width=1\textwidth]{rodo.png}
                \end{figure}
            \end{column}
        \end{columns}
    \end{frame}
	
	\sepframe[title={Tabla de contenidos}]
	
    \frame{\tableofcontents}

	
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Intro al DL --------------------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Intro al aprendizaje profundo}
    
    \begin{frame}
        \sectionpage
    \end{frame}
    
    \subsection{Introducción}
    
    % ----------- Motivación 01 ------------------
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-01}
        \end{figure}
    \end{frame}
    
    % ----------- Motivación 02 ------------------
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-02}
        \end{figure}
    \end{frame}
    
    % ----------- Motivación 03 ------------------
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-03}
        \end{figure}
    \end{frame}

    % ----------- Motivación 04 ------------------
    \begin{frame}{Motivación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{motivation-04}
        \end{figure}
    \end{frame}
    
    % ----------- ¿Qué es el DL? 01 --------------    
    \begin{frame}{¿Qué es el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{quotation}[David Foster (Generative Deep Learning)]
            El aprendizaje profundo (Deep Learning) comprende algoritmos de Machine Learning que (particularmente) utilizan múltiples capas apiladas de unidades de procesamiento para aprender representaciones en un alto nivel sobre datos no estructurados.
        \end{quotation}
    \end{frame}
    
    % ----------- ¿Qué es el DL? 02 --------------
    \begin{frame}{¿Qué es el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{ai-ml-dl}
        \end{figure}
    \end{frame}
    
    % ----------- ¿Qué es el DL? 03 --------------
    \begin{frame}{¿Qué es el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{itemize}
                \item \textbf{Inteligencia artificial:} Cualquier técnica que permita
                a las computadoras emular o imitar el comportamiento humano.
                \item \textbf{Aprendizaje de máquina:} Capacidad de aprender sin ser programado explícitamente, enfoque en los algoritmos y la matemtica.
                \item \textbf{Aprendizaje profundo:} Extrae patrones de datos utilizando redes neuronales.
            \end{itemize}
        \end{center}
    \end{frame}
    
    % ----------- ¿Por qué el DL? 01 ------------
    \begin{frame}{¿Por qué el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{ml-01}
        \end{figure}
    \end{frame}

    % ----------- ¿Por qué el DL? 02 ------------
    \begin{frame}{¿Por qué el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{ml-02}
        \end{figure}
    \end{frame}
    
    % ----------- ¿Por qué el DL? 02 ------------
    \begin{frame}{¿Por qué el \textsl{Deep Learning}?}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{itemize}
                \item La ingeniería de características requiere mucho tiempo, es suceptible a errores y no es escalable consistentemente con datos complejos. Mejor busquemos aprender las características subyacentes directamente de los datos.
                \item Las redes neuronales artificales existen desde hace décadas, pero su predominio actual reside principalmente en los siguientes tres aspectos:
                \begin{enumerate}
                    \item Hardware (GPUs, etc. + Paralelización)
                    \item Software (Frameworks para trabajar con NNs)
                    \item Grandes cantidades de datos
                \end{enumerate}
                \item El aprendizaje profundo está revolucionando muchos campos.
            \end{itemize}
        \end{center}
    \end{frame}
    
    % ----------- Contexto histórico ------------
    \subsection{Contexto histórico}
    
    \begin{frame}{Contexto histórico}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{santiago}
            \caption{Santiago Ramón y Cajal}
        \end{figure}
    \end{frame}
    
    % ----------- Santiago Ramón y Cajal --------
    \begin{frame}{Contexto histórico}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.3\textwidth]{santiago-neuron} $\longrightarrow$
            \includegraphics[width=0.5\textwidth]{neuron}
            
            \let\thefootnote\relax\footnote{{\tiny “Santiago Ramón y Cajal Drawings.” Janelia Research Campus. Accessed July 5, 2024. \href{https://www.janelia.org/archive/santiago-ramón-y-cajal-drawings}{https://www.janelia.org}}}
            
            \let\thefootnote\relax\footnote{{\tiny “Overview of Neuron Structure and Function (Article).” Khan Academy. Accessed July 5, 2024. \href{https://www.khanacademy.org/science/biology/human-biology/neuron-nervous-system/a/overview-of-neuron-structure-and-function}{https://www.khanacademy.org}}}
        \end{figure}
    \end{frame}
    
    % ----------- McCulloch & Pitts -------------
    \begin{frame}{TLU}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{mcculloch-pitts}
            \caption{Warren McCulloch \& Walter Pitts}
        \end{figure}
    \end{frame}
    
    % ----------- TLU ---------------------------
    \begin{frame}{TLU}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$H$};
                \node[output](o) at (5,1){$y$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
                
                \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (-2,3) -- (-0.75,3) node [black,midway,yshift=+0.6cm]{entrada};
                \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (-0.25,3) -- (1,3) node [black,midway,yshift=+0.6cm]{pesos};
                \draw [decorate,decoration={brace,amplitude=10pt},xshift=-4pt,yshift=0pt] (4.5,1.5) -- (5.75,1.5) node [black,midway,yshift=+0.6cm]{salida};
                
                \draw[->] (1,-1.75)node[sum]{$\Sigma$} -- (3,-1.75)[xshift=30]node{$\displaystyle\sum_{i=1}^{n}w_ix_i$};
            \end{tikzpicture}
        \end{center}
    \end{frame}
    
    % -------- Bias y función de activación 01 --
    \begin{frame}{Bias y función de activación}{Intro al aprendizaje profundo}
        La operación matemática que realiza la neurona para la decisión de umbralización se puede escribir como:
        
        $$ f(\textbf{x}) =
        \begin{cases}
            0 & \text{si $\displaystyle\sum_{i}w_ix_i <$ umbral o threshold} \\
            1 & \text{si $\displaystyle\sum_{i}w_ix_i \geq$ umbral o threshold} \\
        \end{cases}$$
        
        donde $i \in \{1, 2, ..., n\}$, y así, $\textbf{x} = (x_1, x_2, ..., x_n)$.
    \end{frame}
    
    % -------- Bias y función de activación 01 --
    \begin{frame}{Bias y función de activación}{Intro al aprendizaje profundo}
        De lo anterior, podemos despejar el umbral y escribirlo como $b$, obteniendo:
        
        $$ f(\textbf{x}) =
        \begin{cases}
            0 & \text{si $\displaystyle\sum_{i}w_ix_i + b < 0$} \\
            1 & \text{si $\displaystyle\sum_{i}w_ix_i + b > 0$} \\
        \end{cases}$$
        
        donde $\textbf{x} = (x_1, x_2, ..., x_n)$ y $i \in \{1, 2, ..., n\}$.

        A esto que escribimos como $b$, también se le conoce como \textbf{bias}, y una interpretación es describir \textit{qué tan susceptible es la neurona a \textbf{dispararse}} (como se exploró en el ejemplo práctico de la identificación de la actividad de Julieta).
    \end{frame}
    
    % -------- Bias y función de activación 03 --
    \begin{frame}{Bias y función de activación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.425\textwidth]{heavyside} $\longrightarrow$
            \includegraphics[width=0.45\textwidth]{sigmoid-plot}
        \end{figure}
    \end{frame}
    
    % ----------- El Perceptrón 01 --------------
    \subsection{Perceptrón}
    
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{rosenblatt}
            \caption{Frank Rosenblatt}
        \end{figure}
    \end{frame}
    
    
    % ----------- El Perceptrón 02 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$\sigma$};
                \node[output](o) at (5,1){$y$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            
            \vspace{20pt}
            ¡Y se agrega un algoritmo formal de entrenamiento!\\(\textit{Backpropagation})
        \end{center}
    \end{frame}
    
    % ----------- Idea de entrenamiento 01 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{training-idea-01}
        \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 02 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{training-idea-02}
    \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 03 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{training-idea-03}
    \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 04 ------
    \begin{frame}{Idea intuitiva de entrenamiento}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{training-idea-04}
        \end{figure}
    \end{frame}
    
    % ----------- Idea de entrenamiento 05 ------
    \begin{frame}{Medición del error}{Intro al aprendizaje profundo}
        Dado el vector $X$, \textbf{¿qué vector ($A, B, C$) se le \textit{parece} más?}
        
        \begin{align*}
            X &= \begin{bmatrix}
                0.5 \\
                0.3 \\
                0.7
            \end{bmatrix}
        \end{align*}
        
        \begin{align*}
            A = \begin{bmatrix}
                0.3 \\
                0.3 \\
                0.3
            \end{bmatrix},
            B = \begin{bmatrix}
                0.6 \\
                0.2 \\
                0.6
            \end{bmatrix},
            C = \begin{bmatrix}
                -0.5 \\
                -0.3 \\
                -0.7
            \end{bmatrix}
        \end{align*}
    \end{frame}
    
    % ----------- Idea de entrenamiento 05 ------
    \begin{frame}{Medición del error}{Intro al aprendizaje profundo}
        Dado el vector $X$, \textbf{¿qué vector ($A, B, C$) se le \textit{parece} más?}
        
        \begin{align*}
            X &= \begin{bmatrix}
                0.5 \\
                0.3 \\
                0.7
            \end{bmatrix}
        \end{align*}
        
        \begin{align*}
            A = \begin{bmatrix}
                0.3 \\
                0.3 \\
                0.3
            \end{bmatrix},
            \colorbox{green!20}{$B = \begin{bmatrix}
                0.6 \\
                0.2 \\
                0.6
            \end{bmatrix}$},
            C = \begin{bmatrix}
                -0.5 \\
                -0.3 \\
                -0.7
            \end{bmatrix}
        \end{align*}
    \end{frame}
    
    % ----------- Idea de entrenamiento 06 ------
    \begin{frame}{Optimización del error}{Intro al aprendizaje profundo}
        \begin{columns}
            % Columna izquierda
            \begin{column}{0.6\textwidth}
                \begin{itemize}
                    \item \textbf{Error:} Es una función.
                    \item \textbf{Optimizar:} Maximizar o minimizar.
                    \item \textbf{Gradiente:} Derivada de una función vectorial, proporciona información sobre máximos o mínimos.
                    \item \textbf{Descenso de gradiente:} Algoritmo para, iterativamente, buscar optimizar una función.
                    \item \textbf{Limitantes:} 
                        \begin{itemize}
                            \item Max's/min's locales.
                            \item Tamaño de salto en gradiente
                        \end{itemize}
                \end{itemize}
            \end{column}
            % Columna derecha
            \begin{column}{0.35\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[width=1\textwidth]{gd}
                \end{figure}
            \end{column}
        \end{columns}
    \end{frame}
    
    % ----------- Observaciones 01 ----------------
    \begin{frame}{Observaciones}{Intro al aprendizaje profundo}
        Hasta este punto, debemos notar que hay algunas observaciones importantes:
        \begin{itemize}
            \item \textbf{TLUs:} 
            \begin{itemize}
                \item No existe un algoritmo de aprendizaje formal → Búsqueda de pesos.
                \item Se limita a propagación hacia adelante (\textit{forward pass/forward propagation})
            \end{itemize}
            \item \textbf{Perceptrón:} Puede utilizar retropropagación, introducido en 1958.
            \item \textbf{Retropropagación:} Algoritmo para realizar ajustes en los valores de los pesos.
            \item \textbf{Limitantes:} Separabilidad lineal.
            \item \textbf{¿Alguna otra observación?}
        \end{itemize}
    \end{frame}
    
    % ----------- Ejercicio 01 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Manzanas vs. Naranjas}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.4\textwidth]{apple-orange}
        \end{figure}
    \end{frame}
    
    % ----------- Lecturas recomendadas 01 ------
    \begin{frame}{Lecturas recomendadas}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Breve historia sobre \colorbox{blue!10}{\href{https://telefonicatech.com/blog/historia-de-la-ia-frank-rosenblatt-y-e}{el perceptrón}}
            \item Post sobre \colorbox{blue!10}{\href{https://lamaquinaoraculo.com/deep-learning/el-perceptron-de-rosenblatt/}{el perceptrón de Rosenblatt}}
            \item Post sobre la \colorbox{blue!10}{\href{https://lamaquinaoraculo.com/deep-learning/la-funcion-de-activacion/}{función de activación}}
            \item \colorbox{blue!10}{\href{https://ploomber.io/blog/threshold/}{Selección de threshold}} para clasificadores binarios
            \item Post sobre \colorbox{blue!10}{\href{https://www.ibm.com/mx-es/topics/neural-networks}{redes neuronales}} por IBM
        \end{itemize}
    \end{frame}
    
    % ----------- Producto matricial 01 ---------
    \subsection{Perceptrón multicapa}
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
        -2 & 1 & 0\\
        -2 & 2 & 1\\
        0 & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
         &  & \\
         &  & \\
         &  & 
        \end{bmatrix}$$
    \end{frame}
    
    % ----------- Producto matricial 02 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            \colorbox{blue!10}{2} & \colorbox{blue!10}{5} & \colorbox{blue!10}{2}\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            \colorbox{blue!10}{-2} & 1 & 0\\
            \colorbox{blue!10}{-2} & 2 & 1\\
            \colorbox{blue!10}{0} & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
           \colorbox{blue!10}{14} &  & \\
            &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(2 \cdot -2) + (5 \cdot -2) + (2 \cdot 0) = -14$$
    \end{frame}
    
    % ----------- Producto matricial 03 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            \colorbox{blue!10}{2} & \colorbox{blue!10}{5} & \colorbox{blue!10}{2}\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            -2 & \colorbox{blue!10}{1} & 0\\
            -2 & \colorbox{blue!10}{2} & 1\\
            0 & \colorbox{blue!10}{0} & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & \colorbox{blue!10}{12} & \\
            &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(2 \cdot 1) + (5 \cdot 2) + (2 \cdot 0) = 12$$
    \end{frame}
    
    % ----------- Producto matricial 04 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            \colorbox{blue!10}{2} & \colorbox{blue!10}{5} & \colorbox{blue!10}{2}\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
        -2 & 1 & \colorbox{blue!10}{0}\\
        -2 & 2 & \colorbox{blue!10}{1}\\
        0 & 0 & \colorbox{blue!10}{3}
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & \colorbox{blue!10}{11}\\
            &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(2 \cdot 0) + (5 \cdot 1) + (2 \cdot 3) = 11$$
    \end{frame}
    
    % ----------- Producto matricial 05 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            \colorbox{blue!10}{1} & \colorbox{blue!10}{0} & \colorbox{blue!10}{-2}\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            \colorbox{blue!10}{-2} & 1 & 0\\
            \colorbox{blue!10}{-2} & 2 & 1\\
            \colorbox{blue!10}{0} & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & 11\\
            \colorbox{blue!10}{-2} &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$(1 \cdot -2) + (0 \cdot -2) + (-2 \cdot 0) = -2$$
    \end{frame}
    
    % ----------- Producto matricial 06 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            \colorbox{blue!10}{1} & \colorbox{blue!10}{0} & \colorbox{blue!10}{-2}\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            \colorbox{blue!10}{-2} & 1 & 0\\
            \colorbox{blue!10}{-2} & 2 & 1\\
            \colorbox{blue!10}{0} & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & 11\\
            \colorbox{blue!10}{-2} &  & \\
            &  & 
        \end{bmatrix}$$
        
        $$\cdots$$
    \end{frame}
    
    % ----------- Producto matricial 07 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        Recordemos cómo opera el producto matricial:
        
        $$\begin{bmatrix}
            2 & 5 & 2\\
            1 & 0 & -2\\
            3 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            -2 & 1 & 0\\
            -2 & 2 & 1\\
            0 & 0 & 3
        \end{bmatrix} = 
        \begin{bmatrix}
            14 & 12 & 11\\
            -2 & 1 & -6\\
            -8 & 5 & 4
        \end{bmatrix}$$
    \end{frame}
    
    % ----------- El Perceptrón 03 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$f$};
                \node[output](o) at (5,1){$\hat{y}$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            \vspace{10pt}
            $$\hat{y} = f\left(\displaystyle\sum_{i}^{n}w_ix_i + b\right)$$
        \end{center}
    \end{frame}
    
    % ----------- El Perceptrón 04 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$f$};
                \node[output](o) at (5,1){$\hat{y}$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            \vspace{10pt}
            $$\displaystyle\sum_{i}^{n}w_ix_i = w_1x_1 + w_2x_2 + \cdots + w_nx_n$$
        \end{center}
    \end{frame}
    
    % ----------- El Perceptrón 05 --------------
    \begin{frame}{El Perceptrón}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{bias}=[draw,shape=circle,minimum size=0.8cm,fill=green!10]
                \tikzstyle{weight}=[draw,shape=rectangle,minimum size=0.5cm,fill=gray!20]
                \tikzstyle{sum}=[draw,shape=circle,minimum size=1cm,fill=yellow!20]
                \tikzstyle{activation}=[draw,shape=rectangle,minimum size=0.5cm,fill=blue!20]
                \tikzstyle{output}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]
                
                
                \node[unit](x1) at (-1.5,2.5){$x_1$};
                \node[unit](x2) at (-1.5,1.5){$x_2$};
                \node(dots) at (-1.5,0.75){\vdots};
                \node[unit](xn) at (-1.5,-0.25){$x_n$};
                \node[bias](b) at (-1.5,-1.25){$b$};
                
                
                \node[weight](w1) at (0.25,2.5){$w_1$};
                \node[weight](w2) at (0.25,1.5){$w_2$};
                \node[weight](wn) at (0.25,-0.25){$w_n$};
                \node[weight](wb) at (0.25,-1.25){$1$};
                
                \node[sum](p) at (2,1){$\Sigma$};
                \node[activation](a) at (3.5,1){$f$};
                \node[output](o) at (5,1){$\hat{y}$};
                
                \draw (x1) -- (w1);
                \draw (x2) -- (w2);
                \draw (xn) -- (wn);
                \draw (b) -- (wb);
                \draw[->] (w1) -- (p);
                \draw[->] (w2) -- (p);
                \draw[->] (wn) -- (p);
                \draw[->] (wb) -- (p);
                \draw[->] (p) -- (a);
                \draw[->] (a) -- (o);
            \end{tikzpicture}
            \vspace{10pt}
            $$\mathbf{w^{T}}\mathbf{x} = [w_1 w_2 \cdots w_n] \begin{bmatrix}
                x_1 \\
                x_2 \\
                \vdots \\
                x_n \\
            \end{bmatrix} = w_1x_1 + w_2x_2 + \cdots + w_nx_n$$
        \end{center}
    \end{frame}
    
    % ----------- Producto matricial 08 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-01}
        \end{figure}
    \end{frame}
    
    % ----------- Producto matricial 09 ---------
    \begin{frame}{Producto matricial}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-02}
        \end{figure}
    \end{frame}
    
    % --------- Composición de funciones 01 -----
    \begin{frame}{Composición de funciones}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-03}
        \end{figure}
    \end{frame}
    
    % --------- Composición de funciones 02 -----
    \begin{frame}{Composición de funciones}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=1\textwidth]{nns-04}
        \end{figure}
    \end{frame}

    % --------- Funciones de activación ---------
    \begin{frame}{Funciones de activación}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{activation-functions}
            $$f(x) = \sigma(x) = \dfrac{1}{1 + e^{-x}} \Rightarrow f'(x) = f(x)(1 - f(x))$$
            $$f(x) = tanh(x) = \dfrac{e^x - e^{-x}}{e^x + e^{-x}} \Rightarrow f'(x) = 1 - f(x)^2$$
        \end{figure}
    \end{frame}
    
    % ----------- El Perceptrón Multicapa 01 ----
    \begin{frame}{El perceptrón multicapa}{Intro al aprendizaje profundo}
        \colorlet{myred}{red!80!black}
        \colorlet{myblue}{blue!80!black}
        \colorlet{mygreen}{green!60!black}
        \colorlet{mydarkred}{myred!40!black}
        \colorlet{mydarkblue}{myblue!40!black}
        \colorlet{mydarkgreen}{mygreen!40!black}
        \tikzstyle{node}=[very thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
        \tikzstyle{connect}=[->,thick,mydarkblue,shorten >=1]
        \tikzset{ % node styles, numbered for easy mapping with \nstyle
            node 1/.style={node,mydarkgreen,draw=mygreen,fill=mygreen!25},
            node 2/.style={node,mydarkblue,draw=myblue,fill=myblue!20},
            node 3/.style={node,mydarkred,draw=myred,fill=myred!20},
        }
        \def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3
        
        \begin{center}
            \begin{tikzpicture}[x=2.4cm,y=1	.2cm]
                \readlist\Nnod{4,3,2} % array of number of nodes per layer
                \readlist\Nstr{n,m,k} % array of string number of nodes per layer
                \readlist\Cstr{x,h^{(\prev)},y} % array of coefficient symbol per layer
                \def\yshift{0.55} % shift last node for dots
                
                % LOOP over LAYERS
                \foreachitem \N \in \Nnod{
                    \def\lay{\Ncnt} % alias of index of current layer
                    \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
                    \foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
                        \x=\lay; \n=\nstyle;
                        \index=(\i<\N?int(\i):"\Nstr[\n]");}] in {1,...,\N}{ % loop over nodes
                        % NODES
                        \node[node \n] (N\lay-\i) at (\x,\y) {$\strut\Cstr[\n]_{\index}$};
                        
                        % CONNECTIONS
                        \ifnumcomp{\lay}{>}{1}{ % connect to previous layer
                            \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                                \draw[white,line width=1.2,shorten >=1] (N\prev-\j) -- (N\lay-\i);
                                \draw[connect] (N\prev-\j) -- (N\lay-\i);
                            }
                            \ifnum \lay=\Nnodlen
                            \draw[connect] (N\lay-\i) --++ (0.5,0); % arrows out
                            \fi
                        }{
                            \draw[connect] (0.5,\y) -- (N\lay-\i); % arrows in
                        }
                        
                    }
                    \path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.6] {$\vdots$}; % dots
                }
                
                % LABELS
                \node[above=0.25,align=center,mydarkgreen] at (N1-1.90) {Capa de \\[-0.2em]entrada};
                \node[above=0.25,align=center,mydarkblue] at (N2-1.90) {Capas\\[-0.2em]ocultas};
                \node[above=0.25,align=center,mydarkred] at (N\Nnodlen-1.90) {Capa de\\[-0.2em]salida};
                
            \end{tikzpicture}
        \end{center}
    \end{frame}
    
    % ----------- El Perceptrón Multicapa 01 ----
    \begin{frame}{El perceptrón multicapa}{Intro al aprendizaje profundo}
        \tikzstyle{node}=[very thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
        \tikzstyle{connect}=[->,thick,mydarkblue,shorten >=1]
        \tikzset{ % node styles, numbered for easy mapping with \nstyle
            node 1/.style={node,mydarkgreen,draw=mygreen,fill=mygreen!25},
            node 2/.style={node,mydarkblue,draw=myblue,fill=myblue!20},
            node 3/.style={node,mydarkred,draw=myred,fill=myred!20},
        }
        \def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3
        \begin{center}
            \begin{tikzpicture}[x=2.4cm,y=1	.2cm]
                \readlist\Nnod{3,3,1} % array of number of nodes per layer
                \readlist\Nstr{n,m,1} % array of string number of nodes per layer
                \readlist\Cstr{x,h^{(\prev)},y} % array of coefficient symbol per layer
                \def\yshift{0.55} % shift last node for dots
                
                % LOOP over LAYERS
                \foreachitem \N \in \Nnod{
                    \def\lay{\Ncnt} % alias of index of current layer
                    \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
                    \foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
                        \x=\lay; \n=\nstyle;
                        \index=(\i<\N?int(\i):"\Nstr[\n]");}] in {1,...,\N}{ % loop over nodes
                        % NODES
                        \node[node \n] (N\lay-\i) at (\x,\y) {$\strut\Cstr[\n]_{\index}$};
                        
                        % CONNECTIONS
                        \ifnumcomp{\lay}{>}{1}{ % connect to previous layer
                            \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                                \draw[white,line width=1.2,shorten >=1] (N\prev-\j) -- (N\lay-\i);
                                \draw[connect] (N\prev-\j) -- (N\lay-\i);
                            }
                            \ifnum \lay=\Nnodlen
                            \draw[connect] (N\lay-\i) --++ (0.5,0); % arrows out
                            \fi
                        }{
                            \draw[connect] (0.5,\y) -- (N\lay-\i); % arrows in
                        }
                        
                    }
                    \path (N\lay-\N) --++ (0,1+\yshift) node[midway,scale=1.6] {$\vdots$}; % dots
                }
                
                % LABELS
                \node[above=0.25,align=center,mydarkgreen] at (N1-1.90) {Capa de \\[-0.2em]entrada};
                \node[above=0.25,align=center,mydarkblue] at (N2-1.90) {Capas\\[-0.2em]ocultas};
                \node[above=0.25,align=center,mydarkred] at (N\Nnodlen-1.90) {Capa de\\[-0.2em]salida};
            \end{tikzpicture}
            \vspace{10pt}\\
            Para $\mathbf{x}^{(i)}=(x^{(i)}_1, x^{(i)}_2, \dots, x^{(i)}_n)$, $y^{(i)}$, tendríamos que la salida es $\hat{y}^{(i)}_1 = f(x^{(i)})$. 
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 01 ----------------
    \subsection{Aprendizaje}
    \begin{frame}{Cuantificación del error}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item La \textbf{función de pérdida (\textit{loss})} de nuestra red neuronal \textit{mide} el \textit{costo} asociado a predicciones incorrectas.
            \item Si observaciones (de entrada y salida) $(x^{(i)}, y^{(i)})$ y consideramos a la salida como función de $x^{(i)}$ y $\mathbf{W}$, entonces las salidas son $\hat{y} = f(x^{(i)}; \mathbf{W})$ y la función de pérdida puede escribirse como:
            $$\mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})$$
            Es decir, una función que mida la salida \textit{real} con la \textit{predicción}.
            \underline{Todo esto para una observación $i$.}
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 02 ----------------
    \begin{frame}{Cuantificación del error}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Para todas las observaciones:
            $$\mathbf{J(W)} = \displaystyle \dfrac{1}{n}\sum_{i=1}^n \mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})$$
            A esta función se le conoce como función de costo o función objetivo (lo que queremos minimizar).
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 03 ----------------
    \begin{frame}{Algunos ejemplos}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item \textit{\textbf{Binary Cross Entropy Loss}}:
            Se puede utilizar con modelos que devuelven como salida una probabilidad entre 0 y 1.
            $$\mathbf{J(W)} = \displaystyle \dfrac{1}{n}\sum_{i=1}^n y^{(i)} \log(f(x^{(i)}; \mathbf{W})) + (1 - y^{(i)}) \log(1 - f(x^{(i)}; \mathbf{W}))$$
            \item \textit{\textbf{Mean Squared Error (MSE) Loss}}:
            Se puede utilizar con modelos de regresión que generan números reales continuos.
            $$\mathbf{J(W)} = \displaystyle \dfrac{1}{n}\sum_{i=1}^n \left( y^{(i)} - f(x^{(i)}; \mathbf{W})\right)^2$$
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 03 ----------------
    \begin{frame}{Optimización del error}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Queremos encontrar los pesos ideales de la red neuronal, los cuales minimizan $\mathbf{J(W)}$, es decir:
            $$\mathbf{W^*} = \displaystyle \operatorname*{argmin}_\mathbf{W} \dfrac{1}{n}\sum_{i=1}^n \mathcal{L}(f(x^{(i)}; \mathbf{W}), y^{(i)})$$
            $$\mathbf{W^*} = \displaystyle \operatorname*{argmin}_\mathbf{W} \mathbf{J(W)}$$
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 04 ----------------
    \begin{frame}{Descenso de gradiente}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Calcular el gradiente $\dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
    \end{frame}
    
    % ----------- Aprendizaje 05 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{$w_1$};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{15pt}
            
            ¿Cómo se calculan los gradientes?\\
            Con la regla de la cadena.
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 06 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{$w_1$};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{\colorbox{yellow!20}{$w_2$}};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{10pt}
            
            $$\dfrac{\partial \mathbf{J(W)}}{\partial \colorbox{yellow!20}{$w_2$} } = \dfrac{\partial \mathbf{J(W)}}{\partial \hat{y}} \cdot \dfrac{\partial \hat{y}}{\partial \colorbox{yellow!20}{$w_2$}}$$
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 07 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{\colorbox{yellow!20}{$w_1$}};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{10pt}
            
            $$\dfrac{\partial \mathbf{J(W)}}{\partial \colorbox{yellow!20}{$w_1$} } = \dfrac{\partial \mathbf{J(W)}}{\partial \hat{y}} \cdot \dfrac{\partial \hat{y}}{\partial \colorbox{yellow!20}{$w_1$}}$$
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 08 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{\colorbox{yellow!20}{$w_1$}};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{10pt}
            
            $$\dfrac{\partial \mathbf{J(W)}}{\partial \colorbox{yellow!20}{$w_1$} } = \dfrac{\partial \mathbf{J(W)}}{\partial \hat{y}} \cdot \dfrac{\partial \hat{y}}{\partial z_1 } \cdot \dfrac{\partial z_1}{\partial \colorbox{yellow!20}{$w_1$}}$$
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 09 ----------------
    \begin{frame}{Retropropagación}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{tikzpicture}[shorten >=1pt]
                \tikzstyle{unit1}=[draw,shape=circle,minimum size=0.8cm,fill=blue!20]
                \tikzstyle{unit2}=[draw,shape=circle,minimum size=0.8cm,fill=green!20]
                \tikzstyle{unit3}=[draw,shape=circle,minimum size=0.8cm,fill=purple!20]                
                
                \node[unit1](x) at (-2.5,0){$x$};
                \node[unit2](z1) at (0,0){$z_1$};
                \node[unit3](yhat) at (2.5,0){$\hat{y}$};
                
                
                \draw[->] (x) -- (z1);
                \draw[->] (z1) -- (yhat);
                
                \draw [decorate,xshift=-4pt,yshift=0pt] (-1.25,0) -- (-1.25,0) node [black,midway,yshift=+0.25cm]{$w_1$};
                \draw [decorate,xshift=-4pt,yshift=0pt] (1.25,0) -- (1.25,0) node [black,midway,yshift=+0.25cm]{$w_2$};
                
                \draw[->] (yhat) -- (4.5,0)[xshift=15]node{$\mathbf{J(W)}$};
            \end{tikzpicture}
            \vspace{15pt}
            
            ¿Cómo se calculan los gradientes?\\
            Con la regla de la cadena.\\
            
            Esto se repite para \textbf{cada peso} en la red neuronal, usando los gradientes de las capas posteriores.
        \end{center}
    \end{frame}
    
    % --------- Learning rate -------------------
    \begin{frame}{Learning rate}{Intro al aprendizaje profundo}
        \begin{center}
            La actualización de pesos está dada por:
            $$\mathbf{W} \leftarrow \mathbf{W} - \colorbox{yellow!20}{$\eta$} \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$$
            \begin{figure}
                \centering
                \includegraphics[width=0.8\textwidth]{lr}
            \end{figure}
        \end{center}
    \end{frame}
    
    % ----------- Aprendizaje 10 ----------------
    \begin{frame}{Descenso de gradiente}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Calcular el gradiente \colorbox{yellow!20}{$\dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}^*$}
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
        \begin{itemize}
            \item $^*$Esto es muy pesado de calcular (computacionalmente).
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 11 ----------------
    \begin{frame}{Descenso de gradiente estocástico}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente estocástico}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Seleccionar observación $i$
                \item \hspace*{8pt} Calcular el gradiente \colorbox{yellow!20}{$\dfrac{\partial \mathbf{J_i(W)}}{\partial \mathbf{W}}^*$}
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
        \begin{itemize}
            \item $^*$Esto es muy sencillo de calcular (computacionalmente), pero es estocástico.
        \end{itemize}
    \end{frame}
    
    % ----------- Aprendizaje 12 ----------------
    \begin{frame}{Descenso de gradiente estocástico}{Intro al aprendizaje profundo}
        \begin{block}{Algoritmo: Descenso de gradiente estocástico - \textit{Mini batches}}
            \begin{enumerate}
                \item Inicializar los pesos aleatoriamente $\sim \mathcal{N}(0,\sigma^2)$
                \item Repetir hasta converger:
                \item \hspace*{8pt} Seleccionar un batch $B$ de observaciones
                \item \hspace*{8pt} Calcular el gradiente \colorbox{yellow!20}{$\dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}} = \frac{1}{B} \sum_{k=1}^B \dfrac{\partial \mathbf{J_k(W)}}{\partial \mathbf{W}}^*$}
                \item \hspace*{8pt} Actualizar los pesos $\mathbf{W} \leftarrow \mathbf{W} - \eta \dfrac{\partial \mathbf{J(W)}}{\partial \mathbf{W}}$
                \item Devolver pesos \textit{óptimos}
            \end{enumerate}
        \end{block}
        \begin{itemize}
            \item $^*$Esto es rápido de calcular (computacionalmente), y da una mejor estimación del gradiente.
        \end{itemize}
    \end{frame}
    
    % ----------- Observaciones 02 ----------------
    \begin{frame}{Observaciones}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Los \textbf{frameworks} para aprendizaje profundo (como TensorFlow, PyTorch, etc.) ya hacen la diferenciación y optimización por nosotros, es decir, ya calculan el gradiente y actualizan los pesos.
            \item Nosotros exploraremos el uso de \textbf{TensorFlow} a través de su API de alto nivel, \textbf{Keras}, para las redes neuronales que estaremos construyendo.
            \item Comenzaremos retomando algunos de los problemas planteados en la sesión anterior.
        \end{itemize}
    \end{frame}
    
    % ----------- TensorFlow --------------------
    \begin{frame}{TensorFlow}{Intro al aprendizaje profundo}
        \begin{center}
            \begin{figure}
                \centering
                \includegraphics[width=0.5\textwidth]{tf}
            \end{figure}
            \colorbox{blue!10}{\href{https://www.tensorflow.org/}{TensorFlow}} es un framework open-source para Machine Learning desarrollado por Google. Utilizado para construir y entrenar redes neuronales artificiales.
        \end{center}
    \end{frame}
    
    % ----------- Ejercicio 02 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Exploración del TensorFlow Playground}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.65\textwidth]{tf-playground}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 03 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Problema de separabilidad lineal}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{xor}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 04 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Exploración con TensorFlow}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{tf}
        \end{figure}
    \end{frame}
    
    % ----------- Lecturas recomendadas 02 ------
    \begin{frame}{Lecturas recomendadas}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Setting the   \colorbox{blue!10}{\href{https://www.jeremyjordan.me/nn-learning-rate/}{learning rate}} of your neural network
            \item \colorbox{blue!10}{\href{https://hmkcode.com/ai/backpropagation-step-by-step/}{Retropropagación}} paso a paso
            \item \colorbox{blue!10}{\href{https://www.tensorflow.org/tutorials}{TensorFlow Tutorials}}
            \item Libro \colorbox{blue!10}{\href{http://neuralnetworksanddeeplearning.com/}{Neural Networks and Deep Learning}}
        \end{itemize}
    \end{frame}
    
    % ----------- Regularización 01 --------------
    \subsection{Regularización}
    
    \begin{frame}{El problema del overfitting}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.85\textwidth]{overfitting}
        \end{figure}
    \end{frame}
    
    % ----------- Regularización 02 --------------
    \begin{frame}{Regularización}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item La \textbf{regularización} consiste en alguna técnica que sirve para evitar que un modelo se sobreajuste.
            \item Es necesaria porque ayuda a mejorar la generalización de nuestro modelo con datos no vistos.
            \item Los métodos de regularización que exploraremos serán:
            \begin{itemize}
                \item \textit{Dropout}
                \item \textit{Early stopping}
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Dropout 01 ---------------------
    \begin{frame}{Dropout}{Intro al aprendizaje profundo}
        \def\layersep{1}
        \def\nodesep{0.75}
        
        \begin{tikzpicture}[
            node/.style={circle, draw, thick},
            ]
            
            \foreach \y in {1,...,5}{
                \node[node] (i\y) at (0,\nodesep*\y) {};
                \node[node, right=\layersep of i\y] (h1\y) {};
                \node[node, right=\layersep of h1\y] (h2\y) {};
            }
            
            \node[node, right=\layersep of h22] (o1) {};
            \node[node, right=\layersep of h24] (o2) {};
            
            \foreach \source in {1,...,5}
            \foreach \dest in {1,...,5}{
                \path[-stealth, thick] (i\source) edge (h1\dest);
                \path[-stealth, thick] (h1\source) edge (h2\dest);
            }
            \foreach \source in {1,...,5}
            \foreach \dest in {1,2}
            \draw[-stealth, thick] (h2\source) -- (o\dest);
            
            \draw[-stealth, thick] (4.2,3*\nodesep) -- node[above,font=\bfseries]{dropout} (6, 3*\nodesep);
            
            % Boundary
            
            \foreach \y in {1,...,5}
            \node[node, right=10em of h2\y] (di\y) {};
            
            \node[red,font=\huge] at (di1) {$\times$};
            \node[red,font=\huge] at (di3) {$\times$};
            
            \foreach \y in {1,...,5}
            \node[node, right=\layersep of di\y] (dh1\y) {};
            
            \node[red,font=\huge] at (dh11) {$\times$};
            \node[red,font=\huge] at (dh13) {$\times$};
            \node[red,font=\huge] at (dh14) {$\times$};
            
            \foreach \y in {1,...,5}
            \node[node, right=\layersep of dh1\y] (dh2\y) {};
            
            \node[red,font=\huge] at (dh22) {$\times$};
            \node[red,font=\huge] at (dh24) {$\times$};
            
            \node[node, right=\layersep of dh22] (do1) {};
            \node[node, right=\layersep of dh24] (do2) {};
            
            \foreach \source in {2,4,5}
            \foreach \dest in {2,5}
            \draw[-stealth, thick] (di\source) -- (dh1\dest);
            
            \foreach \source in {2,5}
            \foreach \dest in {1,3,5}
            \draw[-stealth, thick] (dh1\source) -- (dh2\dest);
            
            \foreach \source in {1,3,5}
            \foreach \dest in {1,2}
            \draw[-stealth, thick] (dh2\source) -- (do\dest);
            
        \end{tikzpicture}
    \end{frame}
    
    % ----------- Dropout 02 ---------------------
    \begin{frame}{Dropout}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item Durante el entrenamiento, establecemos aleatoriamente algunas activaciones en 0
            \begin{itemize}
                \item Típicamente hacemos "drop" del 50\% de activaciones en una capa.
                \item Esto forza a la red a no depender de ningún nodo/neurona.
            \end{itemize}
            \item Podemos realizar el dropout en TensorFlow utilizando la capa \texttt{tf.keras.layers.Dropout(0.5)}, donde el 0.5 puede variar de acuerdo a lo especificado.
        \end{itemize}
    \end{frame}
    
    % ----------- Early stopping 01 --------------
    \begin{frame}{Early stopping}{Intro al aprendizaje profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.45\textwidth]{early-stop-01}
            \includegraphics[width=0.45\textwidth]{early-stop-02}
        \end{figure}
    \end{frame}
    
    % ----------- Early stopping 01 --------------
    \begin{frame}{Early stopping}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item El \textit{Early Stopping} puede ser realizado en TensorFlow de manera sencilla creando un callback (función que se llama en cada iteración durante el entrenamiento de la red neuronal):\\
        \end{itemize}
        
        \texttt{\\model = tf.keras.models.Sequential(...)}\\
        \texttt{callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)}\\
        \texttt{history = model.fit(..., callbacks=[callback])}
    \end{frame}
    
    % ----------- Lecturas recomendadas 03 ------
    \begin{frame}{Lecturas recomendadas}{Intro al aprendizaje profundo}
        \begin{itemize}
            \item \colorbox{blue!10}{\href{https://jmlr.org/papers/v15/srivastava14a.html}{Artículo}} "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
            \item \colorbox{blue!10}{\href{https://www.pinecone.io/learn/regularization-in-neural-networks/}{Regularización en Redes Neuronales}}
            \item \colorbox{blue!10}{\href{https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing}{Vanishing and Exploding Gradients in Neural Network Models:}} Debugging, Monitoring, and Fixing
        \end{itemize}
    \end{frame}
    
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Computer Vision ----------------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Visión computacional profunda}
    
    
    \begin{frame}
        \sectionpage
    \end{frame}
    
    % ----------- Intro a imágenes 01 ------------
    \subsection{Introducción a imágenes}
    \begin{frame}{¿Qué es una imagen?}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.75\textwidth]{img-deer}
        \end{figure}
    \end{frame}
    
    % ----------- Intro a imágenes 02 ------------
    \begin{frame}{¿Qué es una imagen?}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.75\textwidth]{img-lincoln}
            \let\thefootnote\relax\footnote{{\tiny “Tutorial 1: Image Filtering.” AI Stanford. \url{https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html}}}
        \end{figure}
    \end{frame}
    
    % ----------- Intro a imágenes 03 ------------
    \begin{frame}{¿Qué es una imagen?}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.75\textwidth]{img-lena}
            \let\thefootnote\relax\footnote{{\tiny “Tutorial 1: Image Filtering.” AI Stanford. \url{https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html}}}
        \end{figure}
    \end{frame}
    
    % ----------- Intro a imágenes 04 ------------
    \begin{frame}{¿Qué es una imagen?}{Visión computacional profunda}
        \begin{itemize}
            \item Una imagen es un arreglo de pixeles, la cual puede tener 1 o más canales de color. Usualmente:
            \begin{itemize}
                \item 1 canal de color $\rightarrow$ Escala de grises
                \item 3 canales de color $\rightarrow$ Escala RGB
                \item 4 canales de color $\rightarrow$ Escala RGBA
            \end{itemize}
            \item Un pixel puede ser visto como un objeto 5-dimensional $(x, y, r, g, b)$.
        \end{itemize}
    \end{frame}
    
    % ----------- Biología humana 01 -------------
    \begin{frame}{La biología humana}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{img-encroma-01}
            \let\thefootnote\relax\footnote{{\tiny “How EnChroma Color Blind Glasses Work.” EnChroma. \url{http://enchroma.com/technology/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Biología humana 02 -------------
    \begin{frame}{La biología humana}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth]{img-encroma-02}
            \let\thefootnote\relax\footnote{{\tiny “How EnChroma Color Blind Glasses Work.” EnChroma. \url{http://enchroma.com/technology/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Espacios de color --------------
    \begin{frame}{Espacios de color}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{img-color-space}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 05 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Introducción a imágenes}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{img-lena}
        \end{figure}
    \end{frame}
    
    % ----------- Convoluciones 01 ---------------
    \subsection{Convoluciones \& Pooling}
    \begin{frame}{¿Qué es una convolución?}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{1d-conv}
        \end{figure}
    \end{frame}
    
    % ----------- Convoluciones 02 ---------------
    \begin{frame}{¿Qué es una convolución?}{Visión computacional profunda}
        \begin{center}
            \begin{tikzpicture}[
                2d-arr/.style={matrix of nodes, row sep=-\pgflinewidth, column sep=-\pgflinewidth, nodes={draw}}, ampersand replacement=\&
              ]
                
                \matrix (mtr) [2d-arr] {
                    0 \& 1 \& 1 \& |[fill=orange!30]| 1 \& |[fill=orange!30]| 0 \& |[fill=orange!30]| 0 \& 0\\
                    0 \& 0 \& 1 \& |[fill=orange!30]| 1 \& |[fill=orange!30]| 1 \& |[fill=orange!30]| 0 \& 0\\
                    0 \& 0 \& 0 \& |[fill=orange!30]| 1 \& |[fill=orange!30]| 1 \& |[fill=orange!30]| 1 \& 0\\
                    0 \& 0 \& 0 \& 1 \& 1 \& 0 \& 0\\
                    0 \& 0 \& 1 \& 1 \& 0 \& 0 \& 0\\
                    0 \& 1 \& 1 \& 0 \& 0 \& 0 \& 0\\
                    1 \& 1 \& 0 \& 0 \& 0 \& 0 \& 0\\
                };
                
                \node[below=of mtr-5-4] {$\mathbf I$};
                
                \node[right=0.2em of mtr] (str) {$*$};
                
                \matrix (K) [2d-arr, right=0.2em of str, nodes={draw, fill=teal!30}] {
                    1 \& 0 \& 1 \\
                    0 \& 1 \& 0 \\
                    1 \& 0 \& 1 \\
                };
                \node[below=of K-3-2] {$\mathbf K$};
                
                \node[right=0.2em of K] (eq) {$=$};
                
                \matrix (ret) [2d-arr, right=0.2em of eq] {
                    1 \& 4 \& 3 \& |[fill=blue!80!black!30]| 4 \& 1\\
                    1 \& 2 \& 4 \& 3 \& 3\\
                    1 \& 2 \& 3 \& 4 \& 1\\
                    1 \& 3 \& 3 \& 1 \& 1\\
                    3 \& 3 \& 1 \& 1 \& 0\\
                };
                \node[below=of ret-4-3] {$\mathbf{I * K}$};
                
                \draw[dashed, teal] (mtr-1-6.north east) -- (K-1-1.north west);
                \draw[dashed, teal] (mtr-3-6.south east) -- (K-3-1.south west);
                
                \draw[dashed, blue!80!black] (K-1-3.north east) -- (ret-1-4.north west);
                \draw[dashed, blue!80!black] (K-3-3.south east) -- (ret-1-4.south west);
                
                \foreach \i in {1,2,3} {
                    \foreach \j in {4,5,6} {
                        \node[font=\tiny, scale=0.6, shift={(-1.2ex,-2ex)}] at (mtr-\i-\j) {$\times \pgfmathparse{int(mod(\i+\j,2))}\pgfmathresult$};
                    }
                }
                
            \end{tikzpicture}
        \end{center}
    \end{frame}
    
    % ----------- Convoluciones 03 ---------------
    \begin{frame}{¿Qué es una convolución?}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{filter}
        \end{figure}
        \let\thefootnote\relax\footnote{{\tiny “Image Kernels.” Victor Powell. \url{https://setosa.io/ev/image-kernels/}}}
    \end{frame}
    
    % ----------- Pooling 01 ---------------------
    \begin{frame}{Pooling}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{pooling}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 06 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Convoluciones \& Pooling}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{filter}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 01 ------------------------
    \subsection{Redes neuronales convolucionales}
    \begin{frame}{Redes neuronales convolucionales}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{lecun}
            \caption{Yann LeCun}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 02 ------------------------
    \begin{frame}{Redes neuronales convolucionales}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{cnns-01}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 03 ------------------------
    \begin{frame}{Redes neuronales convolucionales}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{cnns-02}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 04 ------------------------
    \begin{frame}{Redes neuronales convolucionales}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{cnns-03}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 05 ------------------------
    \subsection{Clasificadores de imágenes (LeNet, VGG16, etc.)}
    \begin{frame}{LeNet-5}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{classifiers-01}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 06 ------------------------
    \begin{frame}{VGG16}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{classifiers-02}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 07 ------------------------
    \begin{frame}{Resnet50}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{classifiers-03}
        \end{figure}
    \end{frame}
    
    % ----------- CNNs 08 ------------------------
    \begin{frame}{GoogLeNet}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.9\textwidth]{classifiers-04}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 07 ------------------
    \begin{frame}{Ejercicio}{Intro al aprendizaje profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Redes neuronales convolucionales}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{classifiers-01}
        \end{figure}
    \end{frame}
    
    % ----------- Lecturas recomendadas 04 ------
    \begin{frame}{Lecturas recomendadas}{Visión computacional}
        \begin{itemize}
            \item Tutorial 1: \colorbox{blue!10}{\href{https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html}{Image Filtering}}
            \item \colorbox{blue!10}{\href{https://setosa.io/ev/image-kernels/}{Image Kernels Explained Visually}} by Victor Powell
            \item \colorbox{blue!10}{\href{https://gudgud96.github.io/2020/11/25/param-pooling/}{Parameterized Pooling Layers}} by Hao Hao Tan
            \item TensorFlow Tutorials: \colorbox{blue!10}{\href{https://www.tensorflow.org/tutorials/images}{Vision}}
            
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 01 ---------------
    \subsection{Autoencoders}
    \begin{frame}{Reconstrucción de imágenes}{Visión computacional profunda}
        \begin{itemize}
            \item Proceso de generar una imagen de salida a partir de una de entrada, generalmente con el objetivo de restaurar o mejorar la calidad de la imagen original.
            \item En el contexto de los autoencoders y la tarea de denoising, la reconstrucción implica generar una versión limpia y libre de ruido de una imagen ruidosa o de baja calidad.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 02 ---------------
    \begin{frame}{Reconstrucción de imágenes}{Visión computacional profunda}
        \begin{enumerate}
            \item Restauración de la calidad visual.
            \item Preservación de la información relevante.
            \item Mejora de aplicaciones y análisis (eliminación de ruido).
            \item Preservación de características y texturas.
            \item Calidad y experiencia visual.
        \end{enumerate}
    \end{frame}
    
    % ----------- Autoencoders 03 ---------------
    \begin{frame}{Aplicaciones}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.75\textwidth]{xray-denoise}
            \let\thefootnote\relax\footnote{{\tiny “Autoencoder For Denoising Images.” Michel Kana. \url{https://towardsdatascience.com/autoencoder-for-denoising-images-7d63a0831bfd}}}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 04 ---------------
    \begin{frame}{Autoencoders}{Visión computacional profunda}
        ANNs utilizadas para aprender representaciones eficientes de los datos de entrada sin necesidad de etiquetas o supervisión externa.
        \begin{figure}
            \centering
            \includegraphics[width=0.65\textwidth]{autoencoder-keras}
            \let\thefootnote\relax\footnote{{\tiny “Building Autoencoders in Keras.” Keras Blog. \url{https://blog.keras.io/building-autoencoders-in-keras.html}}}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 05 ---------------
    \begin{frame}{Estructura de un autoencoder}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{autoencoder-architecture}
            \let\thefootnote\relax\footnote{{\tiny “From Autoencoder to Beta-VAE.” Lilian Weng. \url{https://lilianweng.github.io/posts/2018-08-12-vae/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 06 ---------------
    \begin{frame}{Funcionamiento de autoencoders}{Visión computacional profunda}
        \begin{itemize}
            \item \textit{\textbf{Encoder:}} Toma los datos de entrada y los transforma en una representación de menor dimensionalidad, también conocida como representación latente. 
            \item \textbf{El objetivo del codificador es...} \\
            Comprimir la información esencial de los datos en esta representación latente. 
            \item \underline{Usualmente}, el codificador está compuesto por capas de neuronas que reducen gradualmente la dimensionalidad de los datos a medida que se propagan a través de la red. Este proceso de reducción dimensional es crucial para extraer las características más importantes de los datos y deshacerse de la información redundante o ruidosa.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 07 ---------------
    \begin{frame}{Funcionamiento de autoencoders}{Visión computacional profunda}
        \begin{itemize}
            \item \textit{\textbf{Decoder:}} Toma la representación latente generada y la reconstruye en una salida que se asemeja lo más posible a la entrada original. 
            \item \textbf{El objetivo del decodificador es...}\\ 
            Descomprimir la representación latente y generar una reconstrucción fiel de los datos de entrada.
            \item \underline{Al igual que el encoder}, el decodificador está compuesto por capas de neuronas, pero en este caso, las capas aumentan gradualmente la dimensionalidad de la representación latente hasta que se obtiene una salida de la misma dimensión que los datos de entrada originales.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 08 ---------------
    \begin{frame}{Funcionamiento de autoencoders}{Visión computacional profunda}
        \begin{itemize}
            \item La idea central es que, al restringir la capacidad de reconstrucción de la red, se obliga al codificador a aprender representaciones más compactas y significativas de los datos. 
            
            \item En otras palabras, se busca que el codificador capture las características más importantes y relevantes de los datos en la representación latente, mientras que el decodificador se encarga de reconstruir los datos de entrada a partir de esa representación latente.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 09 ---------------
    \begin{frame}{Estructura de un autoencoder}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{autoencoder-architecture}
            \let\thefootnote\relax\footnote{{\tiny “From Autoencoder to Beta-VAE.” Lilian Weng. \url{https://lilianweng.github.io/posts/2018-08-12-vae/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 10 ---------------
    \begin{frame}{Aplicaciones de los autoencoders}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.4\textwidth]{autoencoder-mini}
        \end{figure}
        \begin{itemize}
            \item \textbf{Denoising de imágenes:} Se utilizan para eliminar el ruido de las imágenes y reconstruir versiones más limpias.
            \item \textbf{Reducción de dimensionalidad:} Los autoencoders se utilizan para reducir la dimensionalidad de los datos, lo que facilita la visualización y comprensión de datos complejos. Esto es útil en análisis exploratorio de datos, visualización de datos de gran dimensión y clustering.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 11 ---------------
    \begin{frame}{Aplicaciones de los autoencoders}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.4\textwidth]{autoencoder-mini}
        \end{figure}
        \begin{itemize}
            \item \textbf{Generación de contenido:} Pueden generar contenido nuevo y original aprendiendo las características latentes de un conjunto de datos. Esto se utiliza en aplicaciones como la generación de imágenes sintéticas, la creación de música o la generación de texto.
            \item \textbf{Detección de anomalías:} Se utilizan para detectar patrones anormales o inusuales en los datos. Esto se aplica en áreas como la detección de fraudes en transacciones financieras, la detección de intrusiones en sistemas de seguridad o la detección de anomalías en imágenes médicas.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 11 ---------------
    \begin{frame}{Entrenamiento de autoencoders}{Visión computacional profunda}
        \begin{itemize}
            \item \textbf{Aprendizaje:} Durante el entrenamiento de los autoencoders, se utilizan pares de datos de entrada y salida correspondientes. 
            \item La red se entrena para \textbf{minimizar} la diferencia entre la entrada original y la salida reconstruida.
        \end{itemize}
    \end{frame}
    
    % ----------- Ejercicio 08 ------------------
    \begin{frame}{Ejercicio}{Visión computacional profunda}
        \begin{center}
            {\Large \textbf{Ejercicio: Autoencoder básico}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{autoencoder-keras}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 12 ---------------
    \begin{frame}{Limitantes}{Visión computacional profunda}
        \begin{itemize}
            \item \textbf{Incapacidad para capturar información espacial:} Los autoencoders básicos pueden tener dificultades para capturar la estructura espacial de las imágenes, ya que no tienen en cuenta la información de vecindad de los píxeles.
            \item \textbf{Sensibilidad a las transformaciones:} Pueden ser sensibles a las transformaciones geométricas, como la rotación o el desplazamiento de la imagen, lo que puede afectar su capacidad de reconstrucción.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 13 ---------------
    \begin{frame}{Limitantes}{Visión computacional profunda}
        \begin{itemize}
            \item \textbf{Autoencoders convolucionales como solución:} Son una variante de los autoencoders que incorporan capas convolucionales para abordar las limitaciones mencionadas.
            \item \textbf{Ventajas de los autoencoders convolucionales:} Mejoran con la capacidad para capturar algunas características espaciales, preservar la estructura y ser más robustos frente a transformaciones geométricas
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 14 ---------------
    \begin{frame}{Estructura de un autoencoder}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{autoencoder-architecture}
            \let\thefootnote\relax\footnote{{\tiny “From Autoencoder to Beta-VAE.” Lilian Weng. \url{https://lilianweng.github.io/posts/2018-08-12-vae/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 15 ---------------
    \begin{frame}{Estructura de un autoencoder}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.85\textwidth]{conv-autoencoder}
            \let\thefootnote\relax\footnote{{\tiny “IROS 2017: Feature discovery and visualization of robot mission data using convolutional autoencoders and Bayesian nonparametric topic modeling.” \url{https://warp.whoi.edu/iros2017/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 16 ---------------
    \begin{frame}{Denoising autoencoder (DAE/DCAE)}{Visión computacional profunda}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{dcae}
            \let\thefootnote\relax\footnote{{\tiny “Denoising autoencoder (DAE).” \url{https://subscription.packtpub.com/book/data/9781788629416/3/ch03lvl1sec19/denoising-autoencoder-dae}}}
        \end{figure}
    \end{frame}
    
    % ----------- Ejercicio 09 ------------------
    \begin{frame}{Ejercicio}{Visión computacional profunda}
        \begin{center}
            {\Large \textbf{Ejercicio: Autoencoder convolucional}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{conv-autoencoder}
        \end{figure}
    \end{frame}
    
    % ----------- Autoencoders 17 ---------------
    \subsection{Trabajos relacionados y avances recientes}
    
    \begin{frame}{Trabajos relacionados y avances recientes}{Visión computacional profunda}
        Han habido varios trabajos de investigación y avances recientes que han contribuido al desarrollo de nuevas arquitecturas, técnicas de entrenamiento mejoradas y aplicaciones emergentes.
        \begin{columns}
            \begin{column}{0.55\textwidth}
                \begin{itemize}
                    \item \textbf{UNet:} Es ampliamente utilizada en el campo de la segmentación de imágenes, pero también se ha aplicado con éxito en tareas de denoising.
                \end{itemize}
            \end{column}
            \begin{column}{0.4\textwidth}
                \begin{center}
                    \includegraphics[width=\textwidth]{unet-model}
                \end{center}
            \end{column}
        \end{columns}
    \end{frame}
    
    % ----------- Autoencoders 18 ---------------
    \begin{frame}{Trabajos relacionados y avances recientes}{Visión computacional profunda}
        \begin{columns}
            \begin{column}{0.55\textwidth}
                \begin{itemize}
                    \item \textbf{Variational Autoencoders (VAEs):} Los VAEs son una variante de los autoencoders que se utilizan para el aprendizaje de distribuciones latentes. Han demostrado ser efectivos en el denoising de imágenes al aprender representaciones latentes que siguen una distribución probabilística, lo que permite una generación más controlada y realista de imágenes limpias.
                \end{itemize}
            \end{column}
            \begin{column}{0.4\textwidth}
                \begin{center}
                    \includegraphics[width=\textwidth]{vae}
                \end{center}
            \end{column}
        \end{columns}
    \end{frame}
    
    % ----------- Autoencoders 19 ---------------
    \begin{frame}{Trabajos relacionados y avances recientes}{Visión computacional profunda}
        \begin{columns}
            \begin{column}{0.55\textwidth}
                \begin{itemize}
                    \item \textbf{Generative Adversarial Networks (GANs):} Estos modelos aprovechan la capacidad de los GANs para generar imágenes realistas y para aprender representaciones latentes eficientes. Los GANs han demostrado ser efectivos en el denoising y la generación de imágenes de alta calidad, entre otros.
                \end{itemize}
            \end{column}
            \begin{column}{0.4\textwidth}
                \begin{center}
                    \includegraphics[width=\textwidth]{gans-ae}
                \end{center}
            \end{column}
        \end{columns}
    \end{frame}
    
    % ----------- Autoencoders 20 ---------------
    \begin{frame}{Tareas en el campo de visión artificial}{Visión computacional profunda}
        \begin{itemize}
            \item \textbf{Clasificación de imágenes:} La tarea de clasificación de imágenes implica asignar una etiqueta o categoría a una imagen de entrada. Esto implica entrenar un modelo para reconocer y distinguir diferentes objetos, personas o escenas en una imagen.
            \item \textbf{Detección de objetos:} La detección de objetos implica localizar y clasificar múltiples objetos en una imagen. El objetivo es detectar la presencia y la ubicación de objetos específicos en una escena, a menudo utilizando cuadros delimitadores para delinear las regiones donde se encuentran los objetos.
            \item \textbf{Denoising o reconstrucción de imágenes:} Consiste en eliminar o reducir el ruido presente en una imagen, obteniendo una versión más limpia y clara. Esta tarea es relevante en áreas como la fotografía, la medicina y la seguridad.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 21 ---------------
    \begin{frame}{Tareas en el campo de visión artificial}{Visión computacional profunda}
        \begin{itemize}
            \item \textbf{Segmentación semántica:} La segmentación semántica implica asignar una etiqueta a cada píxel de una imagen para identificar y delimitar las diferentes regiones o objetos presentes. El objetivo es comprender la estructura y el contenido de una imagen a nivel de píxel.
            \item \textbf{Detección de rostros:} La detección de rostros es una tarea específica de la visión artificial que implica detectar y localizar los rostros en una imagen. Es ampliamente utilizado en aplicaciones de reconocimiento facial, análisis de emociones y sistemas de seguridad.
            \item \textbf{Reconocimiento y verificación facial:} El reconocimiento facial se refiere a la tarea de identificar y reconocer a una persona específica a partir de una imagen o secuencia de imágenes. La verificación facial se enfoca en verificar si una imagen de rostro coincide con una identidad específica.
        \end{itemize}
    \end{frame}
    
    % ----------- Autoencoders 22 ---------------
    \begin{frame}{Tareas en el campo de visión artificial}{Visión computacional profunda}
        \begin{itemize}
            \item \textbf{Estimación de pose:} La estimación de pose se refiere a la tarea de determinar la posición y orientación de un objeto o persona en una imagen. Esto implica detectar y rastrear las articulaciones o puntos clave en una imagen para comprender la postura y el movimiento.
            \item \textbf{Estimación de profundidad:} La estimación de profundidad implica inferir la información de la distancia o la profundidad de los objetos en una imagen. Es útil en aplicaciones de realidad virtual, conducción autónoma y sistemas de navegación.
            \item \textbf{Super-resolución:} La super-resolución se refiere a aumentar la resolución o la calidad de una imagen de baja resolución. El objetivo es generar una versión de alta resolución que capture más detalles y claridad.
        \end{itemize}
    \end{frame}
    
    % ----------- Lecturas recomendadas 05 ------
    \begin{frame}{Lecturas recomendadas}{Visión computacional}
        \begin{itemize}
            \item \colorbox{blue!10}{\href{https://lilianweng.github.io/posts/2018-08-12-vae/}{From Autoencoder to Beta-VAE}}
            \item \colorbox{blue!10}{\href{https://blog.keras.io/building-autoencoders-in-keras.html}{Building Autoencoders in Keras}}
            \item \colorbox{blue!10}{\href{https://www.codificandobits.com/blog/autoencoders-explicacion-y-tutorial-python/}{Autoencoders: explicación y tutorial en Python}}
            \item \colorbox{blue!10}{\href{https://towardsdatascience.com/autoencoder-for-denoising-images-7d63a0831bfd}{Autoencoder For Denoising Images}}
            \item Medical image denoising using \colorbox{blue!10}{\href{https://arxiv.org/pdf/1608.04667}{convolutional denoising autoencoders}}
            
        \end{itemize}
    \end{frame}

    {
        \setbeamercolor{background canvas}{bg=}
        \includepdf[pages={1-11}]{unet.pdf}
    }
    
    % ----------- Ejercicio 10 ------------------
    \begin{frame}{Ejercicio}{Visión computacional profunda}
        \begin{center}
            {\Large \textbf{Ejercicio: Entrenamiento de U-Net}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{unet-model}
        \end{figure}
    \end{frame}
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Sequence Modeling -------------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % ----------- Anuncio -----------------------    
    \section{Modelado profundo de secuencias}
    
    
    \begin{frame}
        \sectionpage
    \end{frame}
    
    \subsection{Sesión con Mtro. Paco}
    
    \begin{frame}{¡Anuncio!}{Modelado profundo de secuencias}
        \begin{center}
            {\Large \textbf{Próxima sesión: Modelado profundo de secuencias (con Mtro. Paco)}}
        \end{center}
    \end{frame}
    
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Generative Modeling -----------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % ----------- Modelos generativos 01 --------
    \section{Modelado generativo profundo}
    
    
    \begin{frame}
        \sectionpage
    \end{frame}
    
    \subsection{Introducción a modelos generativos}
    
    \begin{frame}{¿Qué son los Modelos Generativos?}{Modelado generativo profundo}
        \begin{itemize}
            \item Un modelo generativo es un tipo de modelo que puede generar nuevos datos a partir de un conjunto de datos de entrenamiento.
            \item A diferencia de los modelos discriminativos, que se centran en predecir etiquetas a partir de características, los modelos generativos buscan aprender la distribución subyacente de los datos para poder generar nuevas muestras.
            \item \textbf{\textit{En esencia...}} Los modelos generativos se enfocan en modelar cómo se distribuyen los datos de forma que puedan generar datos que "parecen" reales.
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 02 --------
    \begin{frame}{¿Qué son los Modelos Generativos?}{Modelado generativo profundo}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{faces-not-exist}
        \end{figure}
    \end{frame}
    
    % ----------- Modelos generativos 03 --------
    \begin{frame}{Modelos generativos vs. discriminativos}{Modelado generativo profundo}
        \begin{itemize}
            \item \textbf{Modelos discriminativos:}
            \begin{itemize}
                \item Se enfocan en la predicción de etiquetas.
                \item Ejemplos: Regresión logística, SVM, redes neuronales feedforward.
            \end{itemize}
            \item \textbf{Modelos generativos:}
            \begin{itemize}
                \item Modelan la distribución conjunta de los datos.
                \item Ejemplos: Redes Bayesianas, GANs, VAEs.
            \end{itemize}
            \item \textbf{\textit{En esencia...}} Los modelos discriminativos responden a preguntas como "¿A qué clase pertenece esta muestra?", mientras que los generativos responden a "¿Cómo se ve una muestra típica de esta clase?".
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 04 --------
    \begin{frame}{¿Qué son los Modelos Generativos?}{Modelado generativo profundo}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{discriminativo}
        \end{figure}
    \end{frame}
    
    % ----------- Modelos generativos 05 --------
    \begin{frame}{Aplicaciones de modelos generativos}{Modelado generativo profundo}
        \begin{itemize}
            \item \textbf{Imágenes:}
            \begin{itemize}
                \item Generación de rostros humanos (e.g., StyleGAN).
                \item Generación de arte abstracto.
            \end{itemize}
            \item \textbf{Texto:}
            \begin{itemize}
                \item Composición automática de párrafos o historias.
                \item Traducción automática y generación de poesía.
            \end{itemize}
            \item \textbf{Música:}
            \begin{itemize}
                \item Composición de melodías.
                \item Generación de pistas basadas en un estilo dado.
            \end{itemize}
            \item \textbf{Videojuegos:}
            \begin{itemize}
                \item Generación de mundos y personajes.
                \item Creación de niveles procedurales.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 06 --------
    \begin{frame}{Ejemplos}{Modelado generativo profundo}
        \begin{center}
            {\Huge \textbf{EJEMPLOS}}
            
            \textbf{Pregunta:} ¿cómo están revolucionando estas industrias? ¿cuáles son las implicaciones éticas y creativas de estas aplicaciones?
        \end{center}
    \end{frame}
    
    % ----------- Modelos generativos 07 --------
    \begin{frame}{Autoencoders (AEs)}{Modelado generativo profundo}
        \begin{itemize}
            \item Un autoencoder es una red neuronal diseñada para aprender una representación comprimida (codificación) de los datos de entrada.
            \item \textbf{Codificador (Encoder):} Reduce la dimensionalidad de los datos.
            \item \textbf{Decodificador (Decoder):} Reconstruye los datos originales a partir de la representación comprimida.
            \item \textbf{\textit{En esencia...}} La idea principal detrás de los autoencoders es aprender a copiar la entrada a la salida, pero a través de una representación más pequeña (la representación latente), lo que obliga al modelo a aprender características esenciales de los datos.
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 08 --------
    \begin{frame}{Estructura de un autoencoder}{Modelado generativo profundo}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{autoencoder-architecture}
            \let\thefootnote\relax\footnote{{\tiny “From Autoencoder to Beta-VAE.” Lilian Weng. \url{https://lilianweng.github.io/posts/2018-08-12-vae/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Modelos generativos 09 --------
    \begin{frame}{Autoencoders}{Modelado generativo profundo}
        \begin{itemize}
            \item \textbf{Reducción de dimensionalidad:} Los autoencoders pueden utilizarse para reducir la dimensionalidad de los datos, similar al Análisis de Componentes Principales (PCA).
            \item \textbf{Detección de anomalías:} Las reconstrucciones defectuosas pueden indicar datos anómalos o inusuales.
            \item \textbf{Generación de datos sintéticos:} Generar nuevas muestras modificando la representación latente.
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 10 --------
    \subsection{Variational Autoencoders (VAEs)}
    
    \begin{frame}{Variational Autoencoders (VAEs)}{Modelado generativo profundo}
        \begin{itemize}
            \item Un VAE es una versión probabilística de un autoencoder que genera nuevas muestras al aprender la distribución subyacente de los datos.
            \item Las principales diferencias con un \textit{AE}:
            \begin{itemize}
                \item \textbf{Codificación estocástica:} Los VAEs aprenden a mapear datos a una distribución (e.g., normal) en lugar de un punto fijo.
                \item \textbf{Regularización:} Introduce una pérdida de regularización para garantizar que la distribución latente sea continua.
            \end{itemize}
            \item \textbf{\textit{En esencia...}} Los VAEs no sólo intentan reconstruir los datos, sino que también buscan aprender una estructura probabilística que permita generar nuevas muestras plausibles al azar.
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 11 --------
    \begin{frame}{Estructura de un variational autoencoder}{Modelado generativo profundo}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{vae-gaussian}
            \let\thefootnote\relax\footnote{{\tiny “From Autoencoder to Beta-VAE.” Lilian Weng. \url{https://lilianweng.github.io/posts/2018-08-12-vae/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Modelos generativos 12 --------
    \begin{frame}{Observaciones (VAEs)}{Modelado generativo profundo}
        \begin{itemize}
            \item En lugar de aprender un vector de características latentes fijo para cada entrada, un VAE aprende los parámetros (media y varianza) de una distribución que representa posibles codificaciones latentes para esa entrada.
            \item A partir de estos parámetros, se toma una muestra para obtener la representación latente que se pasará al decodificador (decoder). Este proceso de muestreo introduce un componente estocástico (aleatorio), que permite que un mismo dato de entrada pueda ser codificado en múltiples puntos del espacio latente, lo que facilita la generación de nuevas muestras similares pero no idénticas.
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 13 --------
    \begin{frame}{Variational Autoencoders (VAEs)}{Modelado generativo profundo}
        \begin{itemize}
            \item \textbf{Generación de nuevos datos:} Los VAEs pueden generar nuevas muestras variando las entradas en la distribución latente.
            \item \textbf{Interpolación y Manipulación de Latentes:} Permite crear transiciones suaves entre diferentes muestras (e.g., morfing de imágenes).
            \item \textbf{\textit{En esencia...}} Los VAEs son potentes para generar datos nuevos y realistas, especialmente en aplicaciones como la generación de rostros o la creación de contenido sintético.
        \end{itemize}
    \end{frame}
    
    % ----------- Ejercicio 11 ------------------
    \begin{frame}{Ejercicio}{Modelado generativo profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Tutoriales de TensorFlow}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{vae-gaussian}
        \end{figure}
    \end{frame}
    
    % ----------- Modelos generativos 14 --------
    \subsection{Redes Generativas Adversariales (GANs)}
    
    \begin{frame}{Redes Generativas Antagónicas/Adversariales (GANs)}{Modelado generativo profundo}
        \begin{itemize}
            \item Una GAN es una clase de redes neuronales en la que dos redes (un generador y un discriminador) compiten en un juego de suma cero.
            \item \textbf{Componentes principales:} 
            \begin{itemize}
                \item \textbf{Generador ($G$):} Crea datos falsos que intentan parecer reales.
                \item \textbf{Discriminador ($D$):} Intenta distinguir entre datos reales y datos generados por el generador.
            \end{itemize}
            \item \textbf{\textit{En esencia...}} El generador y el discriminador se entrenan simultáneamente en un proceso adversarial. El generador intenta mejorar sus datos generados para engañar al discriminador, mientras que el discriminador intenta mejorar su capacidad para distinguir entre datos reales y generados.
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 15 --------
    \begin{frame}{Estructura de una GAN}{Modelado generativo profundo}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{gans}
        \end{figure}
    \end{frame}
    
    % ----------- Modelos generativos 16 --------
    \begin{frame}{Funcionamiento de GANs)}{Modelado generativo profundo}
        \begin{itemize}
            \item \textbf{Entrenamiento:} 
            \begin{itemize}
                \item \textbf{Generación de datos:} El generador crea una muestra a partir de ruido.
                \item \textbf{Evaluación del discriminador:} El discriminador evalúa si la muestra es real o falsa.
                \item \textbf{Retroalimentación:} El generador y el discriminador actualizan sus pesos en función de la retroalimentación recibida.
            \end{itemize}
            \item \textbf{Objetivo:} 
            \begin{itemize}
                \item \textbf{Generador:} Minimizar la probabilidad de que el discriminador identifique las muestras como falsas.
                \item \textbf{Discriminador:} Maximizar la precisión al clasificar muestras como reales o falsas.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 17 --------
    \begin{frame}{Variantes de GANs)}{Modelado generativo profundo}
        \begin{itemize}
            \item \textbf{Deep Convolutional GANs (DCGANs):} Utilizan capas convolucionales en lugar de capas completamente conectadas para mejorar la calidad de las imágenes generadas. \textbf{Aplicaciones:} Generación de imágenes de alta resolución y texturas más realistas.
            \item \textbf{Conditional GANs (cGANs):} Incorporan información adicional (etiquetas) en el proceso de generación para controlar el tipo de datos generados. \textbf{Aplicaciones:} Generación de imágenes en categorías específicas, como rostros con expresiones particulares o escenas con características predefinidas.
            \item \textbf{CycleGAN:}  Permite la conversión de imágenes entre dos dominios diferentes sin necesidad de pares de imágenes coincidentes, utilizando ciclos de consistencia para asegurar que la conversión sea realista en ambas direcciones. \textbf{Aplicaciones:} Transferencia de estilo entre fotos y pinturas, traducción de imágenes entre estaciones o entre distintos tipos de objetos.
        \end{itemize}
    \end{frame}
    
    % ----------- Modelos generativos 18 --------
    \begin{frame}{Aplicaciones de las GANs)}{Modelado generativo profundo}
        \begin{itemize}
            \item \textit{Generación de imágenes realistas:} Creación de imágenes fotorrealistas a partir de ruido aleatorio.
            \item \textit{Deepfakes:} Sustitución de rostros en videos con gran realismo.
            \item \textit{Estilo de transferencia:} Aplicación de estilos artísticos a imágenes.
            \item \textit{Aumento de datos:} Generación de datos sintéticos para mejorar los conjuntos de datos en problemas de aprendizaje automático.
        \end{itemize}
    \end{frame}
    
    % ----------- Ejercicio 12 ------------------
    \begin{frame}{Ejercicio}{Modelado generativo profundo}
        \begin{center}
            {\Large \textbf{Ejercicio: Tutoriales de TensorFlow}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{gans}
        \end{figure}
    \end{frame}
    
    % ----------- Lecturas recomendadas 06 ------
    \begin{frame}{Lecturas recomendadas}{Modelado generativo profundo}
        \begin{itemize}
            \item \colorbox{blue!10}{\href{https://lilianweng.github.io/posts/2018-08-12-vae/}{From Autoencoder to Beta-VAE}}
            \item \colorbox{blue!10}{\href{https://www.cs.us.es/~fsancho/Blog/posts/VAE.md}{Variational AutoEncoder}}
            \item \colorbox{blue!10}{\href{https://learnopencv.com/variational-autoencoder-in-tensorflow/}{Variational Autoencoder}} in TensorFlow
            \item \colorbox{blue!10}{\href{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}{What are Diffusion Models?}}
            \item \colorbox{blue!10}{\href{https://developer.ibm.com/articles/generative-adversarial-networks-explained/}{Generative adversarial networks explained
            }}
        \end{itemize}
    \end{frame}
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Panorama actual y futuro ------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Panorama actual y futuro}
    
    \begin{frame}
        \sectionpage
    \end{frame}
    
    % ----------- Panorama actual 01 ------------
    \begin{frame}{Evolución de la IA}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{1950s - 1970s: Nacimiento de la IA}
            \begin{itemize}
                \item \textbf{1943: Primeras Ideas Inspiradas en el Cerebro.} \\Warren McCulloch y Walter Pitts proponen el primer modelo computacional de una red neuronal, sentando las bases teóricas para la IA inspirada en el funcionamiento del cerebro humano.
    
                \item \textbf{1950: Alan Turing y el Test de Turing.} \\Alan Turing publica "Computing Machinery and Intelligence", donde introduce el concepto de "máquinas pensantes" y el Test de Turing para evaluar la inteligencia de las máquinas.
                
                \item \textbf{1956: La Conferencia de Dartmouth.} \\Considerada el nacimiento oficial de la IA como campo académico, John McCarthy, Marvin Minsky, Nathaniel Rochester, y Claude Shannon organizan una conferencia donde acuñan el término "Inteligencia Artificial".
    
                \item \textbf{1960s: Primeros Sistemas de IA.} \\Desarrollo de programas pioneros como ELIZA (un bot conversacional) y SHAKEY (el primer robot móvil con capacidad para razonar sobre sus acciones).
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 02 ------------
    \begin{frame}{Evolución de la IA}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{1980s - 1990s: Primer Invierno de la IA y Resurgimiento}
            \begin{itemize}
                \item \textbf{1970s - 1980s: Invierno de la IA.} \\\textit{Desilusión y retroceso:} Los resultados limitados y el alto costo computacional llevan a una disminución en la financiación y el interés por la IA, un período conocido como el "Invierno de la IA".
                
                \item \textbf{1980s: Resurgimiento con \textit{Expert Systems}.} \\\textit{Sistemas expertos:} Aumento del interés en sistemas basados en reglas que emulan la toma de decisiones humana en dominios específicos, como MYCIN en medicina.
                
                
                \item \textbf{Finales de 1980s: Introducción del aprendizaje profundo} \\\textit{Backpropagation (Retropropagación):} Se populariza como un método eficiente para entrenar redes neuronales multicapa, marcando un hito en la IA.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 03 ------------
    \begin{frame}{Evolución de la IA}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{1990s - 2010s: Expansión y dominio del Deep Learning}
            \begin{itemize}
                \item \textbf{1997: Deep Blue vence a Garry Kasparov.} \\Deep Blue de IBM derrota al campeón mundial de ajedrez Garry Kasparov, demostrando la capacidad de las máquinas para superar a humanos en tareas complejas.
                
                \item \textbf{2000s: Auge de los datos y el poder computacional.} \\\textit{Big Data:} La proliferación de datos digitales y la mejora en el poder computacional permiten entrenar modelos más grandes y complejos. \textit{Internet y Cloud Computing:} La IA se beneficia de la creciente disponibilidad de datos en línea y el acceso a recursos computacionales en la nube.
                
                \item \textbf{2012: AlexNet y la revolución del Deep Learning.} \\AlexNet gana el concurso ImageNet con una red neuronal convolucional profunda, marcando el inicio de la revolución del deep learning en visión por computadora.
                
                \item \textbf{2014: Generative Adversarial Networks (GANs).} \\Ian Goodfellow introduce las GANs, que permiten generar datos sintéticos realistas, revolucionando áreas como la creación de imágenes.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 04 ------------
    \begin{frame}{Evolución de la IA}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{2010s - Presente: Transformers y la nueva era de la IA}
            \begin{itemize}
                \item \textbf{2017: Introducción de Transformers.} \\Vaswani et al. presentan el Transformer en el artículo "Attention is All You Need", revolucionando el procesamiento de lenguaje natural y superando a los RNNs en muchas tareas.
                
                \item \textbf{2018: BERT y el avance en NLP.} \\\textit{Big Data:} BERT (Bidirectional Encoder Representations from Transformers) redefine el estado del arte en NLP con un enfoque bidireccional, mejorando la comprensión contextual.
                
                \item \textbf{2020s: Modelos de Lenguaje de Gran Escala (LLMs).} \\\textit{GPT-3:} Los modelos de lenguaje como GPT-3 de OpenAI muestran capacidades asombrosas en la generación de texto, abriendo nuevas posibilidades y desafíos éticos.
                
                \item \textbf{IA Multimodal y Modelos de Difusión.} \\\textit{Modelos Multimodales:} Integración de múltiples tipos de datos (texto, imagen, audio) en un solo modelo. \textit{Modelos de Difusión:} Innovación en la generación de imágenes y otros datos complejos, ampliando el horizonte de la creatividad artificial.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 05 ------------
    \begin{frame}{Factores clave en la evolución de la IA}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{Big Data: }
            \begin{itemize}
                \item La abundancia de datos ha permitido entrenar modelos más complejos y precisos.
            \end{itemize}
            \item \textbf{Potencia computacional:}
            \begin{itemize}
                \item \textbf{GPUs y TPUs:} Facilitación del entrenamiento de redes profundas.
                \item \textbf{Cloud computing:} Acceso a recursos computacionales a gran escala.
            \end{itemize}
            \item \textbf{Algoritmos y optimización:}
            \begin{itemize}
                \item \textbf{Retropropagación y optimizadores:} Mejoras en la eficiencia y precisión del entrenamiento de redes neuronales.
            \end{itemize}
            \item \textbf{Democratización de la IA:}
            \begin{itemize}
                \item \textbf{Contenido técnico:} El contenido técnico especializado es más accesible.
                \item \textbf{Plataformas, repositorios y ecosistemas:} Creación de plataformas y ambientes completos que permiten hacer uso y re-entrenamiento de modelos robustos de forma sencilla, así como acceso a grandes conjuntos de datos para entrenamiento.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 06 ------------
    \begin{frame}{Transformers y su revolución}{Panorama actual y futuro}
        \begin{itemize}
            \item Es un modelo de deep learning que procesa secuencias de datos en paralelo utilizando mecanismos de autoatención.
            \item \textbf{Autoatención:} Permite a los modelos enfocarse en diferentes partes de una secuencia simultáneamente, mejorando la eficiencia y precisión.
            \item \textbf{Arquitectura:}
            \begin{itemize}
                \item \textbf{Encoder-decoder:} Transformadores clásicos para traducción automática.
                \item \textbf{Self-Attention (Autoatención):} Clave para manejar dependencias a largo plazo.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 07 ------------
    \begin{frame}{Transformers y su revolución}{Panorama actual y futuro}
        \begin{figure}
            \centering
            \includegraphics[width=0.5\textwidth]{transformer}
        \end{figure}
    \end{frame}
    
    % ----------- Panorama actual 08 ------------
    \begin{frame}{Impacto y aplicaciones de los Transformers}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{Procesamiento de Lenguaje Natural (NLP):} 
            \begin{itemize}
                \item \textbf{BERT (2018):} Representación bidireccional del lenguaje que mejoró la comprensión contextual.
                \item \textbf{GPT (Generative Pretrained Transformer):} Generación de texto coherente y fluido.
            \end{itemize}
            \item \textbf{Visión por computadora:} 
            \begin{itemize}
                \item \textbf{Vision Transformers (ViT):} Aplicación de transformers para tareas de clasificación de imágenes.
            \end{itemize}
            \item \textbf{Variantes avanzadas:} 
            \begin{itemize}
                \item \textbf{GPT-3 (2020):} Uno de los modelos de lenguaje más grandes, con capacidad para generar texto coherente y realizar tareas complejas.
                \item \textbf{Transformers Multimodales:} Integración de texto, imagen, y otras modalidades en un solo modelo.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 09 ------------
    \begin{frame}{Impacto y aplicaciones de los Transformers}{Panorama actual y futuro}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{transformer-models}
        \end{figure}
    \end{frame}
    
    % ----------- Panorama actual 10 ------------
    \begin{frame}{Modelos de lenguaje de gran escala (LLMs)}{Panorama actual y futuro}
        \begin{itemize}
            \item Son modelos entrenados en grandes volúmenes de texto para generar, comprender, y manipular lenguaje natural.
            \item \textbf{Características:}
            \begin{itemize}
                \item \textbf{Pre-entrenamiento en datasets masivos:} Aprende patrones generales del lenguaje antes de ser ajustado para tareas específicas.
                \item \textbf{Capacidades avanzadas:} Desde completar texto hasta realizar traducciones y análisis sentimentales.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 11 ------------
    \begin{frame}{Aplicaciones clave de LLMs}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{Generación de texto:} Creación de contenido automatizado, desde artículos hasta código.
            \item \textbf{Chatbots y asistentes virtuales:} Asistentes como ChatGPT, que pueden sostener conversaciones coherentes.
            \item \textbf{Traducción automática:} Mejora en la precisión y fluidez en traducciones entre diferentes idiomas.
            \item \textbf{Asistencia en programación:} Herramientas que ayudan a los desarrolladores a escribir y depurar código.
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 12 ------------
    \begin{frame}{Desafíos y consideraciones éticas}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{Sesgos en modelos:} Los LLMs pueden reflejar y amplificar sesgos presentes en los datos de entrenamiento.
            \item \textbf{Uso ético y responsable:} Implicaciones de confiar en modelos que pueden generar información incorrecta o sesgada.
            \item \textbf{Tendencias futuras:}
            \begin{itemize}
                \item \textbf{Modelos multimodales:} Expansión hacia modelos que pueden manejar múltiples tipos de datos simultáneamente.
                \item \textbf{Optimización y sostenibilidad:} Reducción del costo computacional y mejora en la eficiencia.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Ejercicio 13 ------------------
    \begin{frame}{Ejercicio}{Panorama actual y futuro}
        \begin{center}
            {\Large \textbf{Ejercicio: Exploración de HuggingFace}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{hf}
        \end{figure}
    \end{frame}
    
    % ----------- Panorama actual 13 ------------
    \begin{frame}{Estructura de modelos generativos}{Panorama actual y futuro}
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{generative-overview}
            \let\thefootnote\relax\footnote{{\tiny “What are Diffusion Models?” Lilian Weng. \url{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}}}
        \end{figure}
    \end{frame}
    
    % ----------- Panorama actual 14 ------------
    \begin{frame}{Modelos de difusión}{Panorama actual y futuro}
        \begin{itemize}
            \item Son modelos generativos que aprenden a generar datos simulando el proceso inverso de difundir ruido a través de datos.
            \item \textbf{Funcionamiento:}
            \begin{itemize}
                \item \textbf{Proceso de difusión:} Agregación gradual de ruido a los datos de entrenamiento.
                \item \textbf{Proceso inverso:} El modelo aprende a revertir este ruido, generando datos realistas.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 15 ------------
    \begin{frame}{Ejemplos y aplicaciones de modelos de difusión}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{Generación de Imágenes:}
            \begin{itemize}
                \item \textbf{DALL-E 2:} Generación de imágenes a partir de descripciones textuales.
                \item \textbf{Stable Diffusion:} Generación de imágenes de alta calidad con control sobre el proceso creativo.
            \end{itemize}
            \item \textbf{Simulaciones realistas:} Creación de mundos virtuales y simulaciones precisas para aplicaciones en videojuegos y simulación científica.
        \end{itemize}
    \end{frame}
    
    % ----------- Panorama actual 16 ------------
    \begin{frame}{Desafíos y consideraciones éticas}{Panorama actual y futuro}
        \begin{itemize}
            \item \textbf{Controlabilidad:} Desafíos en el control preciso de la salida generada por los modelos.
            \item \textbf{Complejidad computacional:} La generación puede ser costosa en términos de tiempo y recursos.
            \item \textbf{Potencial futuro:}
            \begin{itemize}
                \item \textbf{Ampliación a otras modalidades:} Aplicación en generación de audio, video, y otros tipos de datos.
                \item \textbf{Innovaciones en creatividad artificial:} Implicaciones para el arte, el diseño, y la innovación tecnológica.
            \end{itemize}
        \end{itemize}
    \end{frame}
    
    % ----------- Ejercicio 14 ------------------
    \begin{frame}{Ejercicio}{Panorama actual y futuro}
        \begin{center}
            {\Large \textbf{Ejercicio: Exploración de Dream Studio}}
        \end{center}
        \begin{figure}
            \centering
            \includegraphics[width=0.6\textwidth]{stability}
        \end{figure}
    \end{frame}
    
    % ----------- Lecturas recomendadas 07 ------
    \begin{frame}{Lecturas recomendadas}{Panorama actual y futuro}
        \begin{itemize}
            \item \colorbox{blue!10}{\href{https://www.datacamp.com/tutorial/how-transformers-work}{How Transformers Work:}} A Detailed Exploration of Transformer Architecture            
            \item \colorbox{blue!10}{\href{https://huggingface.co/learn/nlp-course/es/chapter0/1}{NLP Course}} by HuggingFace
            \item \colorbox{blue!10}{\href{https://www.youtube.com/watch?v=SZorAJ4I-sA}{Transformers, explained:}} Understand the model behind GPT, BERT, and T5
            \item \colorbox{blue!10}{\href{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}{What are Diffusion Models?}}
            \item \colorbox{blue!10}{\href{https://beta.dreamstudio.ai/generate}{Dream Studio}} by stability.ai
        \end{itemize}
    \end{frame}
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    
    % ----------- Empaquetado de modelos --------
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Empaquetado de modelos}
    
    \begin{frame}
        \sectionpage
    \end{frame}
	
\end{document}

%%% Local Variables:
%%% mode: lualatex
%%% TeX-master: Rodolfo Ferro
%%% End:
